import {
  Array_exports,
  BootStatus,
  BucketQueue_exports,
  Context_exports,
  Deferred_exports,
  EVENTLOG_META_TABLE,
  Effect_exports,
  EncodedWithMeta,
  EventSequenceNumber_exports,
  Exit_exports,
  FiberHandle_exports,
  FiberMap_exports,
  Hash_exports,
  IntentionalShutdownCause,
  LS_DEV,
  Layer_exports,
  LeaderAheadError,
  LeaderPullCursor,
  LiveStoreEvent_exports,
  MigrationsReport,
  Option_exports,
  PayloadUpstreamAdvance,
  PayloadUpstreamRebase,
  Queue_exports,
  ROOT,
  SYNC_STATUS_TABLE,
  Schedule_exports,
  Schema_exports,
  SqliteError,
  Stream_exports,
  Subscribable_exports,
  SubscriptionRef_exports,
  SyncState,
  TRACE_VERBOSE,
  Tracer_exports,
  Transferable_exports,
  TreeFormatter,
  UnexpectedError,
  casesHandled,
  clientDefault,
  compare,
  eventlogMetaTable,
  eventlogSystemTables,
  getEventDef,
  getExecStatementsFromMaterializer,
  hashMaterializerResults,
  isDevEnv,
  isNotUndefined,
  liveStoreStorageFormatVersion,
  liveStoreVersion,
  makeMaterializerHash,
  makeMeshNode,
  merge,
  mesh_schema_exports,
  migrateDb,
  migrateTable,
  mod_exports,
  mod_exports4 as mod_exports2,
  nanoid,
  pipe,
  prepareBindValues,
  prettyBytes,
  rematerializeFromEventlog,
  sessionChangesetMetaTable,
  shouldNeverHappen,
  sql,
  sqlite_db_helper_exports,
  syncstate_exports,
  system_tables_exports
} from "./chunk-J7UPD3II.js";
import {
  __export,
  __privateAdd,
  __privateGet,
  __privateMethod,
  __privateSet,
  __publicField
} from "./chunk-DWA4UIM3.js";

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/connection.js
var configureConnection = (sqliteDb, { foreignKeys, lockingMode }) => execSql(
  sqliteDb,
  // We use the WAL journal mode is significantly faster in most scenarios than the traditional rollback journal mode.
  // It specifically significantly improves write performance. However, when using the WAL journal mode, transactions
  // that involve changes against multiple ATTACHed databases are atomic for each database but are not atomic
  // across all databases as a set. Additionally, it is not possible to change the page size after entering WAL mode,
  // whether on an empty database or by using VACUUM or the backup API. To change the page size, we must switch to the
  // rollback journal mode.
  //
  // When connected to an in-memory database, the WAL journal mode option is ignored because an in-memory database can
  // only be in either the MEMORY or OFF options. By default, an in-memory database is in the MEMORY option, which
  // means that it stores the rollback journal in volatile RAM. This saves disk I/O but at the expense of safety and
  // integrity. If the thread using SQLite crashes in the middle of a transaction, then the database file will very
  // likely go corrupt.
  sql`
    -- disable WAL until we have it working properly
    -- PRAGMA journal_mode=WAL;
    PRAGMA page_size=8192;
    PRAGMA foreign_keys=${foreignKeys ? "ON" : "OFF"};
    ${lockingMode === void 0 ? "" : sql`PRAGMA locking_mode=${lockingMode};`}
  `,
  {}
);
var execSql = (sqliteDb, sql2, bind) => {
  const bindValues = prepareBindValues(bind, sql2);
  return Effect_exports.try({
    try: () => sqliteDb.execute(sql2, bindValues),
    catch: (cause) => new SqliteError({ cause, query: { bindValues, sql: sql2 }, code: cause.code })
  }).pipe(
    Effect_exports.asVoid,
    // Effect.logDuration(`@livestore/common:execSql:${sql}`),
    Effect_exports.withSpan(`@livestore/common:execSql`, {
      attributes: { "span.label": sql2, sql: sql2, bindValueKeys: Object.keys(bindValues) }
    })
  );
};
var execSqlPrepared = (sqliteDb, sql2, bindValues) => {
  return Effect_exports.try({
    try: () => sqliteDb.execute(sql2, bindValues),
    catch: (cause) => new SqliteError({ cause, query: { bindValues, sql: sql2 }, code: cause.code })
  }).pipe(
    Effect_exports.asVoid,
    // Effect.logDuration(`@livestore/common:execSqlPrepared:${sql}`),
    Effect_exports.withSpan(`@livestore/common:execSqlPrepared`, {
      attributes: {
        "span.label": sql2,
        sql: sql2,
        bindValueKeys: Object.keys(bindValues)
      }
    })
  );
};

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/types.js
var InitialSyncOptionsSkip = Schema_exports.TaggedStruct("Skip", {});
var InitialSyncOptionsBlocking = Schema_exports.TaggedStruct("Blocking", {
  timeout: Schema_exports.Union(Schema_exports.DurationFromMillis, Schema_exports.Number)
});
var InitialSyncOptions = Schema_exports.Union(InitialSyncOptionsSkip, InitialSyncOptionsBlocking);
var LeaderThreadCtx = class extends Context_exports.Tag("LeaderThreadCtx")() {
};

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/eventlog.js
var eventlog_exports = {};
__export(eventlog_exports, {
  getBackendHeadFromDb: () => getBackendHeadFromDb,
  getClientHeadFromDb: () => getClientHeadFromDb,
  getEventsSince: () => getEventsSince,
  getSyncBackendCursorInfo: () => getSyncBackendCursorInfo,
  initEventlogDb: () => initEventlogDb,
  insertIntoEventlog: () => insertIntoEventlog,
  updateBackendHead: () => updateBackendHead,
  updateSyncMetadata: () => updateSyncMetadata
});

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/sql-queries/misc.js
var objectEntries = (obj) => Object.entries(obj);

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/sql-queries/types.js
var isValidWhereOp = (op) => {
  const validWhereOps = [">", "<", "="];
  return validWhereOps.includes(op);
};

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/sql-queries/sql-queries.js
var insertRow = ({ tableName, columns, values, options = { orReplace: false } }) => {
  const stmt = insertRowPrepared({
    tableName,
    columns,
    options: { orReplace: options == null ? void 0 : options.orReplace, keys: Object.keys(values) }
  });
  return [stmt, makeBindValues({ columns, values })];
};
var insertRowPrepared = ({ tableName, columns, options = { orReplace: false } }) => {
  const keys = (options == null ? void 0 : options.keys) ?? Object.keys(columns);
  const keysStr = keys.join(", ");
  const valuesStr = keys.map((key) => `$${key}`).join(", ");
  return sql`INSERT ${options.orReplace ? "OR REPLACE " : ""}INTO ${tableName} (${keysStr}) VALUES (${valuesStr})`;
};
var updateRows = ({ columns, tableName, updateValues: updateValues_, where }) => {
  const updateValues = filterUndefinedFields(updateValues_);
  if (Object.keys(updateValues).length === 0) {
    return [sql`select 1`, {}];
  }
  const updateValueStr = Object.keys(updateValues).map((columnName) => `${columnName} = $update_${columnName}`).join(", ");
  const bindValues = {
    ...makeBindValues({ columns, values: updateValues, variablePrefix: "update_" }),
    ...makeBindValues({ columns, values: where, variablePrefix: "where_", skipNil: true })
  };
  const whereSql = buildWhereSql({ where });
  const whereModifier = whereSql === "" ? "" : `WHERE ${whereSql}`;
  return [sql`UPDATE ${tableName} SET ${updateValueStr} ${whereModifier}`, bindValues];
};
var makeBindValues = ({ columns, values, variablePrefix = "", skipNil }) => {
  const codecMap = pipe(columns, objectEntries, Array_exports.map(([columnName, columnDef]) => [
    columnName,
    (value) => {
      if (columnDef.nullable === true && (value === null || value === void 0))
        return null;
      const res = Schema_exports.encodeEither(columnDef.schema)(value);
      if (res._tag === "Left") {
        const parseErrorStr = TreeFormatter.formatErrorSync(res.left);
        const expectedSchemaStr = String(columnDef.schema.ast);
        console.error(`Error making bind values for SQL query for column "${columnName}".

Expected schema: ${expectedSchemaStr}

Error: ${parseErrorStr}

Value:`, value);
        debugger;
        throw res.left;
      } else {
        return res.right;
      }
    }
  ]), Object.fromEntries);
  return pipe(Object.entries(values).filter(([, value]) => skipNil !== true || value !== null && value !== void 0).flatMap(([columnName, value]) => {
    const codec = codecMap[columnName] ?? shouldNeverHappen(`No codec found for column "${columnName}"`);
    if (typeof value === "object" && value !== null && "op" in value) {
      switch (value.op) {
        case "in": {
          return value.val.map((value2, i) => [`${variablePrefix}${columnName}_${i}`, codec(value2)]);
        }
        case "=":
        case ">":
        case "<": {
          return [[`${variablePrefix}${columnName}`, codec(value.val)]];
        }
        default: {
          throw new Error(`Unknown op: ${value.op}`);
        }
      }
    } else {
      return [[`${variablePrefix}${columnName}`, codec(value)]];
    }
  }), Object.fromEntries);
};
var buildWhereSql = ({ where }) => {
  const getWhereOp = (columnName, value) => {
    if (value === null) {
      return `IS NULL`;
    } else if (typeof value === "object" && typeof value.op === "string" && isValidWhereOp(value.op)) {
      return `${value.op} $where_${columnName}`;
    } else if (typeof value === "object" && typeof value.op === "string" && value.op === "in") {
      return `in (${value.val.map((_, i) => `$where_${columnName}_${i}`).join(", ")})`;
    } else {
      return `= $where_${columnName}`;
    }
  };
  return pipe(where, objectEntries, Array_exports.map(([columnName, value]) => `${columnName} ${getWhereOp(columnName, value)}`), Array_exports.join(" AND "));
};
var filterUndefinedFields = (obj) => {
  return Object.fromEntries(Object.entries(obj).filter(([, value]) => value !== void 0));
};

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/eventlog.js
var initEventlogDb = (dbEventlog) => Effect_exports.gen(function* () {
  for (const tableDef of eventlogSystemTables) {
    yield* migrateTable({
      db: dbEventlog,
      behaviour: "create-if-not-exists",
      tableAst: tableDef.sqliteDef.ast,
      skipMetaTable: true
    });
  }
  yield* execSql(dbEventlog, sql`INSERT INTO ${SYNC_STATUS_TABLE} (head)
          SELECT ${ROOT.global}
          WHERE NOT EXISTS (SELECT 1 FROM ${SYNC_STATUS_TABLE})`, {});
});
var getEventsSince = (since) => Effect_exports.gen(function* () {
  const { dbEventlog, dbState } = yield* LeaderThreadCtx;
  const query = eventlogMetaTable.where("seqNumGlobal", ">=", since.global).asSql();
  const pendingEventsRaw = dbEventlog.select(query.query, prepareBindValues(query.bindValues, query.query));
  const pendingEvents = Schema_exports.decodeUnknownSync(eventlogMetaTable.rowSchema.pipe(Schema_exports.Array))(pendingEventsRaw);
  const sessionChangesetRows = sessionChangesetMetaTable.where("seqNumGlobal", ">=", since.global).asSql();
  const sessionChangesetRowsRaw = dbState.select(sessionChangesetRows.query, prepareBindValues(sessionChangesetRows.bindValues, sessionChangesetRows.query));
  const sessionChangesetRowsDecoded = Schema_exports.decodeUnknownSync(sessionChangesetMetaTable.rowSchema.pipe(Schema_exports.Array))(sessionChangesetRowsRaw);
  return pendingEvents.map((eventlogEvent) => {
    const sessionChangeset = sessionChangesetRowsDecoded.find((readModelEvent) => readModelEvent.seqNumGlobal === eventlogEvent.seqNumGlobal && readModelEvent.seqNumClient === eventlogEvent.seqNumClient);
    return EncodedWithMeta.make({
      name: eventlogEvent.name,
      args: eventlogEvent.argsJson,
      seqNum: { global: eventlogEvent.seqNumGlobal, client: eventlogEvent.seqNumClient },
      parentSeqNum: { global: eventlogEvent.parentSeqNumGlobal, client: eventlogEvent.parentSeqNumClient },
      clientId: eventlogEvent.clientId,
      sessionId: eventlogEvent.sessionId,
      meta: {
        sessionChangeset: sessionChangeset && sessionChangeset.changeset !== null ? {
          _tag: "sessionChangeset",
          data: sessionChangeset.changeset,
          debug: sessionChangeset.debug
        } : { _tag: "unset" },
        syncMetadata: eventlogEvent.syncMetadataJson,
        materializerHashLeader: Option_exports.none(),
        materializerHashSession: Option_exports.none()
      }
    });
  }).filter((_) => compare(_.seqNum, since) > 0).sort((a, b) => compare(a.seqNum, b.seqNum));
});
var getClientHeadFromDb = (dbEventlog) => {
  const res = dbEventlog.select(sql`select seqNumGlobal, seqNumClient from ${EVENTLOG_META_TABLE} order by seqNumGlobal DESC, seqNumClient DESC limit 1`)[0];
  return res ? { global: res.seqNumGlobal, client: res.seqNumClient } : ROOT;
};
var getBackendHeadFromDb = (dbEventlog) => {
  var _a;
  return ((_a = dbEventlog.select(sql`select head from ${SYNC_STATUS_TABLE}`)[0]) == null ? void 0 : _a.head) ?? ROOT.global;
};
var updateBackendHead = (dbEventlog, head) => dbEventlog.execute(sql`UPDATE ${SYNC_STATUS_TABLE} SET head = ${head.global}`);
var insertIntoEventlog = (eventEncoded, dbEventlog, eventDefSchemaHash, clientId, sessionId) => Effect_exports.gen(function* () {
  if (LS_DEV && eventEncoded.parentSeqNum.global !== ROOT.global) {
    const parentEventExists = dbEventlog.select(`SELECT COUNT(*) as count FROM ${EVENTLOG_META_TABLE} WHERE seqNumGlobal = ? AND seqNumClient = ?`, [eventEncoded.parentSeqNum.global, eventEncoded.parentSeqNum.client])[0].count === 1;
    if (parentEventExists === false) {
      shouldNeverHappen(`Parent mutation ${eventEncoded.parentSeqNum.global},${eventEncoded.parentSeqNum.client} does not exist`);
    }
  }
  yield* execSql(dbEventlog, ...insertRow({
    tableName: EVENTLOG_META_TABLE,
    columns: eventlogMetaTable.sqliteDef.columns,
    values: {
      seqNumGlobal: eventEncoded.seqNum.global,
      seqNumClient: eventEncoded.seqNum.client,
      parentSeqNumGlobal: eventEncoded.parentSeqNum.global,
      parentSeqNumClient: eventEncoded.parentSeqNum.client,
      name: eventEncoded.name,
      argsJson: eventEncoded.args ?? {},
      clientId,
      sessionId,
      schemaHash: eventDefSchemaHash,
      syncMetadataJson: eventEncoded.meta.syncMetadata
    }
  }));
});
var updateSyncMetadata = (items) => Effect_exports.gen(function* () {
  const { dbEventlog } = yield* LeaderThreadCtx;
  for (let i = 0; i < items.length; i++) {
    const event = items[i];
    yield* execSql(dbEventlog, ...updateRows({
      tableName: EVENTLOG_META_TABLE,
      columns: eventlogMetaTable.sqliteDef.columns,
      where: { seqNumGlobal: event.seqNum.global, seqNumClient: event.seqNum.client },
      updateValues: { syncMetadataJson: event.meta.syncMetadata }
    }));
  }
});
var getSyncBackendCursorInfo = (remoteHead) => Effect_exports.gen(function* () {
  const { dbEventlog } = yield* LeaderThreadCtx;
  if (remoteHead === ROOT.global)
    return Option_exports.none();
  const EventlogQuerySchema = Schema_exports.Struct({
    syncMetadataJson: Schema_exports.parseJson(Schema_exports.Option(Schema_exports.JsonValue))
  }).pipe(Schema_exports.pluck("syncMetadataJson"), Schema_exports.Array, Schema_exports.head);
  const syncMetadataOption = yield* Effect_exports.sync(() => dbEventlog.select(sql`SELECT syncMetadataJson FROM ${EVENTLOG_META_TABLE} WHERE seqNumGlobal = ${remoteHead} ORDER BY seqNumClient ASC LIMIT 1`)).pipe(Effect_exports.andThen(Schema_exports.decode(EventlogQuerySchema)), Effect_exports.map(Option_exports.flatten), Effect_exports.orDie);
  return Option_exports.some({
    cursor: { global: remoteHead, client: clientDefault },
    metadata: syncMetadataOption
  });
}).pipe(Effect_exports.withSpan("@livestore/common:eventlog:getSyncBackendCursorInfo", { attributes: { remoteHead } }));

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/leader-worker-devtools.js
var bootDevtools = (options) => Effect_exports.gen(function* () {
  if (options.enabled === false) {
    return;
  }
  const { syncProcessor, extraIncomingMessagesQueue, clientId, storeId } = yield* LeaderThreadCtx;
  yield* listenToDevtools({
    incomingMessages: Stream_exports.fromQueue(extraIncomingMessagesQueue),
    sendMessage: () => Effect_exports.void
  }).pipe(Effect_exports.tapCauseLogPretty, Effect_exports.forkScoped);
  const { node, persistenceInfo, mode } = yield* options.boot;
  yield* node.listenForChannel.pipe(Stream_exports.filter((res) => mod_exports2.isChannelName.devtoolsClientLeader(res.channelName, { storeId, clientId }) && res.mode === mode), Stream_exports.tap(({ channelName, source }) => Effect_exports.gen(function* () {
    const channel = yield* node.makeChannel({
      target: source,
      channelName,
      schema: { listen: mod_exports2.Leader.MessageToApp, send: mod_exports2.Leader.MessageFromApp },
      mode
    });
    const sendMessage = (message) => channel.send(message).pipe(Effect_exports.withSpan("@livestore/common:leader-thread:devtools:sendToDevtools"), Effect_exports.interruptible, Effect_exports.ignoreLogged);
    const syncState = yield* syncProcessor.syncState;
    const mergeCounter = syncProcessor.getMergeCounter();
    yield* syncProcessor.pull({ cursor: { mergeCounter, eventNum: syncState.localHead } }).pipe(Stream_exports.tap(({ payload }) => sendMessage(mod_exports2.Leader.SyncPull.make({ payload, liveStoreVersion }))), Stream_exports.runDrain, Effect_exports.forkScoped);
    yield* listenToDevtools({
      incomingMessages: channel.listen.pipe(Stream_exports.flatten(), Stream_exports.orDie),
      sendMessage,
      persistenceInfo
    });
  }).pipe(Effect_exports.tapCauseLogPretty, Effect_exports.forkScoped)), Stream_exports.runDrain);
}).pipe(Effect_exports.withSpan("@livestore/common:leader-thread:devtools:boot"));
var listenToDevtools = ({ incomingMessages, sendMessage, persistenceInfo }) => Effect_exports.gen(function* () {
  const { syncBackend, makeSqliteDb: makeSqliteDb2, dbState, dbEventlog, shutdownStateSubRef, shutdownChannel, syncProcessor, clientId, devtools } = yield* LeaderThreadCtx;
  const subscriptionFiberMap = yield* FiberMap_exports.make();
  const handledRequestIds = /* @__PURE__ */ new Set();
  yield* incomingMessages.pipe(Stream_exports.tap((decodedEvent) => Effect_exports.gen(function* () {
    const { requestId } = decodedEvent;
    const reqPayload = { requestId, liveStoreVersion, clientId };
    if (decodedEvent._tag === "LSD.Leader.Disconnect") {
      return;
    }
    if (handledRequestIds.has(requestId)) {
      return;
    }
    handledRequestIds.add(requestId);
    switch (decodedEvent._tag) {
      case "LSD.Leader.Ping": {
        yield* sendMessage(mod_exports2.Leader.Pong.make({ ...reqPayload }));
        return;
      }
      case "LSD.Leader.SnapshotReq": {
        const snapshot = dbState.export();
        yield* sendMessage(mod_exports2.Leader.SnapshotRes.make({ snapshot, ...reqPayload }));
        return;
      }
      case "LSD.Leader.LoadDatabaseFile.Request": {
        const { data } = decodedEvent;
        let tableNames;
        try {
          const tmpDb = yield* makeSqliteDb2({ _tag: "in-memory" });
          tmpDb.import(data);
          const tableNameResults = tmpDb.select(`select name from sqlite_master where type = 'table'`);
          tableNames = new Set(tableNameResults.map((_) => _.name));
          tmpDb.close();
        } catch (cause) {
          yield* Effect_exports.logError(`Error importing database file`, cause);
          yield* sendMessage(mod_exports2.Leader.LoadDatabaseFile.Error.make({
            ...reqPayload,
            cause: { _tag: "unexpected-error", cause }
          }));
          return;
        }
        try {
          if (tableNames.has(system_tables_exports.EVENTLOG_META_TABLE)) {
            yield* SubscriptionRef_exports.set(shutdownStateSubRef, "shutting-down");
            dbEventlog.import(data);
            dbState.destroy();
          } else if (tableNames.has(system_tables_exports.SCHEMA_META_TABLE) && tableNames.has(system_tables_exports.SCHEMA_EVENT_DEFS_META_TABLE)) {
            yield* SubscriptionRef_exports.set(shutdownStateSubRef, "shutting-down");
            dbState.import(data);
            dbEventlog.destroy();
          } else {
            yield* sendMessage(mod_exports2.Leader.LoadDatabaseFile.Error.make({
              ...reqPayload,
              cause: { _tag: "unsupported-database" }
            }));
            return;
          }
          yield* sendMessage(mod_exports2.Leader.LoadDatabaseFile.Success.make({ ...reqPayload }));
          yield* shutdownChannel.send(IntentionalShutdownCause.make({ reason: "devtools-import" })) ?? Effect_exports.void;
          return;
        } catch (cause) {
          yield* Effect_exports.logError(`Error importing database file`, cause);
          yield* sendMessage(mod_exports2.Leader.LoadDatabaseFile.Error.make({
            ...reqPayload,
            cause: { _tag: "unexpected-error", cause }
          }));
          return;
        }
      }
      case "LSD.Leader.ResetAllData.Request": {
        const { mode } = decodedEvent;
        yield* SubscriptionRef_exports.set(shutdownStateSubRef, "shutting-down");
        dbState.destroy();
        if (mode === "all-data") {
          dbEventlog.destroy();
        }
        yield* sendMessage(mod_exports2.Leader.ResetAllData.Success.make({ ...reqPayload }));
        yield* shutdownChannel.send(IntentionalShutdownCause.make({ reason: "devtools-reset" })) ?? Effect_exports.void;
        return;
      }
      case "LSD.Leader.DatabaseFileInfoReq": {
        if (persistenceInfo === void 0) {
          console.log("[@livestore/common:leader-thread:devtools] persistenceInfo is required for this request");
          return;
        }
        const dbSizeQuery = `SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size();`;
        const dbFileSize = dbState.select(dbSizeQuery, void 0)[0].size;
        const eventlogFileSize = dbEventlog.select(dbSizeQuery, void 0)[0].size;
        yield* sendMessage(mod_exports2.Leader.DatabaseFileInfoRes.make({
          state: { fileSize: dbFileSize, persistenceInfo: persistenceInfo.state },
          eventlog: { fileSize: eventlogFileSize, persistenceInfo: persistenceInfo.eventlog },
          ...reqPayload
        }));
        return;
      }
      case "LSD.Leader.EventlogReq": {
        const eventlog = dbEventlog.export();
        yield* sendMessage(mod_exports2.Leader.EventlogRes.make({ eventlog, ...reqPayload }));
        return;
      }
      case "LSD.Leader.CommitEventReq": {
        yield* syncProcessor.pushPartial({
          event: decodedEvent.eventEncoded,
          clientId: `devtools-${clientId}`,
          sessionId: `devtools-${clientId}`
        });
        yield* sendMessage(mod_exports2.Leader.CommitEventRes.make({ ...reqPayload }));
        return;
      }
      case "LSD.Leader.SyncHistorySubscribe": {
        const { subscriptionId } = decodedEvent;
        if (syncBackend !== void 0) {
          yield* syncBackend.pull(Option_exports.none()).pipe(Stream_exports.map((_) => _.batch), Stream_exports.flattenIterables, Stream_exports.tap(({ eventEncoded, metadata }) => sendMessage(mod_exports2.Leader.SyncHistoryRes.make({
            eventEncoded,
            metadata,
            subscriptionId,
            ...reqPayload,
            requestId: nanoid(10)
          }))), Stream_exports.runDrain, Effect_exports.interruptible, Effect_exports.tapCauseLogPretty, FiberMap_exports.run(subscriptionFiberMap, subscriptionId));
        }
        return;
      }
      case "LSD.Leader.SyncHistoryUnsubscribe": {
        const { requestId: requestId2 } = decodedEvent;
        console.log("LSD.SyncHistoryUnsubscribe", requestId2);
        yield* FiberMap_exports.remove(subscriptionFiberMap, requestId2);
        return;
      }
      case "LSD.Leader.SyncingInfoReq": {
        const syncingInfo = mod_exports2.Leader.SyncingInfo.make({
          enabled: syncBackend !== void 0,
          metadata: (syncBackend == null ? void 0 : syncBackend.metadata) ?? {}
        });
        yield* sendMessage(mod_exports2.Leader.SyncingInfoRes.make({ syncingInfo, ...reqPayload }));
        return;
      }
      case "LSD.Leader.NetworkStatusSubscribe": {
        if (syncBackend !== void 0) {
          const { subscriptionId } = decodedEvent;
          yield* Effect_exports.sleep(1e3);
          yield* Stream_exports.zipLatest(syncBackend.isConnected.changes, devtools.enabled ? devtools.syncBackendLatchState.changes : Stream_exports.make({ latchClosed: false })).pipe(Stream_exports.tap(([isConnected, { latchClosed }]) => sendMessage(mod_exports2.Leader.NetworkStatusRes.make({
            networkStatus: { isConnected, timestampMs: Date.now(), latchClosed },
            subscriptionId,
            ...reqPayload,
            requestId: nanoid(10)
          }))), Stream_exports.runDrain, Effect_exports.interruptible, Effect_exports.tapCauseLogPretty, FiberMap_exports.run(subscriptionFiberMap, subscriptionId));
        }
        return;
      }
      case "LSD.Leader.NetworkStatusUnsubscribe": {
        const { requestId: requestId2 } = decodedEvent;
        yield* FiberMap_exports.remove(subscriptionFiberMap, requestId2);
        return;
      }
      case "LSD.Leader.SyncHeadSubscribe": {
        const { subscriptionId } = decodedEvent;
        yield* syncProcessor.syncState.changes.pipe(Stream_exports.tap((syncState) => sendMessage(mod_exports2.Leader.SyncHeadRes.make({
          local: syncState.localHead,
          upstream: syncState.upstreamHead,
          subscriptionId,
          ...reqPayload,
          requestId: nanoid(10)
        }))), Stream_exports.runDrain, Effect_exports.interruptible, Effect_exports.tapCauseLogPretty, FiberMap_exports.run(subscriptionFiberMap, subscriptionId));
        return;
      }
      case "LSD.Leader.SyncHeadUnsubscribe": {
        const { subscriptionId } = decodedEvent;
        yield* FiberMap_exports.remove(subscriptionFiberMap, subscriptionId);
        return;
      }
      case "LSD.Leader.SetSyncLatch.Request": {
        const { closeLatch } = decodedEvent;
        if (devtools.enabled === false)
          return;
        if (closeLatch === true) {
          yield* devtools.syncBackendLatch.close;
        } else {
          yield* devtools.syncBackendLatch.open;
        }
        yield* SubscriptionRef_exports.set(devtools.syncBackendLatchState, { latchClosed: closeLatch });
        yield* sendMessage(mod_exports2.Leader.SetSyncLatch.Success.make({ ...reqPayload }));
        return;
      }
      default: {
        yield* Effect_exports.logWarning(`TODO implement devtools message`, decodedEvent);
      }
    }
  }).pipe(Effect_exports.withSpan(`@livestore/common:leader-thread:onDevtoolsMessage:${decodedEvent._tag}`))), UnexpectedError.mapToUnexpectedErrorStream, Stream_exports.runDrain);
});

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/materialize-event.js
var makeMaterializeEvent = ({ schema, dbState, dbEventlog }) => Effect_exports.gen(function* () {
  const eventDefSchemaHashMap = new Map(
    // TODO Running `Schema.hash` can be a bottleneck for larger schemas. There is an opportunity to run this
    // at build time and lookup the pre-computed hash at runtime.
    // Also see https://github.com/Effect-TS/effect/issues/2719
    [...schema.eventsDefsMap.entries()].map(([k, v]) => [k, Schema_exports.hash(v.schema)])
  );
  return (eventEncoded, options) => Effect_exports.gen(function* () {
    const skipEventlog = (options == null ? void 0 : options.skipEventlog) ?? false;
    const eventName = eventEncoded.name;
    const { eventDef, materializer } = getEventDef(schema, eventName);
    const execArgsArr = getExecStatementsFromMaterializer({
      eventDef,
      materializer,
      dbState,
      event: { decoded: void 0, encoded: eventEncoded }
    });
    const materializerHash = isDevEnv() ? Option_exports.some(hashMaterializerResults(execArgsArr)) : Option_exports.none();
    if (materializerHash._tag === "Some" && eventEncoded.meta.materializerHashSession._tag === "Some" && eventEncoded.meta.materializerHashSession.value !== materializerHash.value) {
      yield* UnexpectedError.make({
        cause: `Materializer hash mismatch detected for event "${eventEncoded.name}".`,
        note: `Please make sure your event materializer is a pure function without side effects.`
      });
    }
    const session = dbState.session();
    for (const { statementSql, bindValues } of execArgsArr) {
      yield* execSqlPrepared(dbState, statementSql, bindValues);
    }
    const changeset = session.changeset();
    session.finish();
    yield* execSql(dbState, ...insertRow({
      tableName: system_tables_exports.SESSION_CHANGESET_META_TABLE,
      columns: system_tables_exports.sessionChangesetMetaTable.sqliteDef.columns,
      values: {
        seqNumGlobal: eventEncoded.seqNum.global,
        seqNumClient: eventEncoded.seqNum.client,
        // NOTE the changeset will be empty (i.e. null) for no-op events
        changeset: changeset ?? null,
        debug: LS_DEV ? execArgsArr : null
      }
    }));
    if (skipEventlog === false) {
      const eventName2 = eventEncoded.name;
      const eventDefSchemaHash = eventDefSchemaHashMap.get(eventName2) ?? shouldNeverHappen(`Unknown event definition: ${eventName2}`);
      yield* insertIntoEventlog(eventEncoded, dbEventlog, eventDefSchemaHash, eventEncoded.clientId, eventEncoded.sessionId);
    } else {
    }
    return {
      sessionChangeset: changeset ? {
        _tag: "sessionChangeset",
        data: changeset,
        debug: LS_DEV ? execArgsArr : null
      } : { _tag: "no-op" },
      hash: materializerHash
    };
  }).pipe(Effect_exports.withSpan(`@livestore/common:leader-thread:materializeEvent`, {
    attributes: {
      eventName: eventEncoded.name,
      eventNum: eventEncoded.seqNum,
      "span.label": `${EventSequenceNumber_exports.toString(eventEncoded.seqNum)} ${eventEncoded.name}`
    }
  }));
});
var rollback = ({ dbState, dbEventlog, eventNumsToRollback }) => Effect_exports.gen(function* () {
  const rollbackEvents = dbState.select(sql`SELECT * FROM ${system_tables_exports.SESSION_CHANGESET_META_TABLE} WHERE (seqNumGlobal, seqNumClient) IN (${eventNumsToRollback.map((id) => `(${id.global}, ${id.client})`).join(", ")})`).map((_) => ({
    seqNum: { global: _.seqNumGlobal, client: _.seqNumClient },
    changeset: _.changeset,
    debug: _.debug
  })).toSorted((a, b) => EventSequenceNumber_exports.compare(a.seqNum, b.seqNum));
  for (let i = rollbackEvents.length - 1; i >= 0; i--) {
    const { changeset } = rollbackEvents[i];
    if (changeset !== null) {
      dbState.makeChangeset(changeset).invert().apply();
    }
  }
  const eventNumPairChunks = Array_exports.chunksOf(100)(eventNumsToRollback.map((seqNum) => `(${seqNum.global}, ${seqNum.client})`));
  for (const eventNumPairChunk of eventNumPairChunks) {
    dbState.execute(sql`DELETE FROM ${system_tables_exports.SESSION_CHANGESET_META_TABLE} WHERE (seqNumGlobal, seqNumClient) IN (${eventNumPairChunk.join(", ")})`);
  }
  for (const eventNumPairChunk of eventNumPairChunks) {
    dbEventlog.execute(sql`DELETE FROM ${system_tables_exports.EVENTLOG_META_TABLE} WHERE (seqNumGlobal, seqNumClient) IN (${eventNumPairChunk.join(", ")})`);
  }
}).pipe(Effect_exports.withSpan("@livestore/common:LeaderSyncProcessor:rollback", {
  attributes: { count: eventNumsToRollback.length }
}));

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/LeaderSyncProcessor.js
var makeLeaderSyncProcessor = ({ schema, dbEventlogMissing, dbEventlog, dbState, dbStateMissing, initialBlockingSyncContext, onError, params, testing }) => Effect_exports.gen(function* () {
  const syncBackendPushQueue = yield* BucketQueue_exports.make();
  const localPushBatchSize = params.localPushBatchSize ?? 10;
  const backendPushBatchSize = params.backendPushBatchSize ?? 50;
  const syncStateSref = yield* SubscriptionRef_exports.make(void 0);
  const isClientEvent = (eventEncoded) => {
    const { eventDef } = getEventDef(schema, eventEncoded.name);
    return eventDef.options.clientOnly;
  };
  const connectedClientSessionPullQueues = yield* makePullQueueSet;
  const currentLocalPushGenerationRef = { current: 0 };
  const mergeCounterRef = { current: dbStateMissing ? 0 : yield* getMergeCounterFromDb(dbState) };
  const mergePayloads = /* @__PURE__ */ new Map();
  const ctxRef = {
    current: void 0
  };
  const localPushesQueue = yield* BucketQueue_exports.make();
  const localPushesLatch = yield* Effect_exports.makeLatch(true);
  const pullLatch = yield* Effect_exports.makeLatch(true);
  const pushHeadRef = { current: EventSequenceNumber_exports.ROOT };
  const advancePushHead = (eventNum) => {
    pushHeadRef.current = EventSequenceNumber_exports.max(pushHeadRef.current, eventNum);
  };
  const push = (newEvents, options) => {
    var _a;
    return Effect_exports.gen(function* () {
      if (newEvents.length === 0)
        return;
      yield* validatePushBatch(newEvents, pushHeadRef.current);
      advancePushHead(newEvents.at(-1).seqNum);
      const waitForProcessing = (options == null ? void 0 : options.waitForProcessing) ?? false;
      const generation = currentLocalPushGenerationRef.current;
      if (waitForProcessing) {
        const deferreds = yield* Effect_exports.forEach(newEvents, () => Deferred_exports.make());
        const items = newEvents.map((eventEncoded, i) => [eventEncoded, deferreds[i], generation]);
        yield* BucketQueue_exports.offerAll(localPushesQueue, items);
        yield* Effect_exports.all(deferreds);
      } else {
        const items = newEvents.map((eventEncoded) => [eventEncoded, void 0, generation]);
        yield* BucketQueue_exports.offerAll(localPushesQueue, items);
      }
    }).pipe(Effect_exports.withSpan("@livestore/common:LeaderSyncProcessor:push", {
      attributes: {
        batchSize: newEvents.length,
        batch: TRACE_VERBOSE ? newEvents : void 0
      },
      links: ((_a = ctxRef.current) == null ? void 0 : _a.span) ? [{ _tag: "SpanLink", span: ctxRef.current.span, attributes: {} }] : void 0
    }));
  };
  const pushPartial = ({ event: { name, args }, clientId, sessionId }) => Effect_exports.gen(function* () {
    const syncState2 = yield* syncStateSref;
    if (syncState2 === void 0)
      return shouldNeverHappen("Not initialized");
    const { eventDef } = getEventDef(schema, name);
    const eventEncoded = new LiveStoreEvent_exports.EncodedWithMeta({
      name,
      args,
      clientId,
      sessionId,
      ...EventSequenceNumber_exports.nextPair(syncState2.localHead, eventDef.options.clientOnly)
    });
    yield* push([eventEncoded]);
  }).pipe(Effect_exports.catchTag("LeaderAheadError", Effect_exports.orDie));
  const boot = Effect_exports.gen(function* () {
    var _a, _b, _c;
    const span = yield* Effect_exports.currentSpan.pipe(Effect_exports.orDie);
    const otelSpan = yield* Tracer_exports.currentOtelSpan.pipe(Effect_exports.catchAll(() => Effect_exports.succeed(void 0)));
    const { devtools, shutdownChannel } = yield* LeaderThreadCtx;
    const runtime = yield* Effect_exports.runtime();
    ctxRef.current = {
      otelSpan,
      span,
      devtoolsLatch: devtools.enabled ? devtools.syncBackendLatch : void 0,
      runtime
    };
    const initialLocalHead = dbEventlogMissing ? EventSequenceNumber_exports.ROOT : getClientHeadFromDb(dbEventlog);
    const initialBackendHead = dbEventlogMissing ? EventSequenceNumber_exports.ROOT.global : getBackendHeadFromDb(dbEventlog);
    if (initialBackendHead > initialLocalHead.global) {
      return shouldNeverHappen(`During boot the backend head (${initialBackendHead}) should never be greater than the local head (${initialLocalHead.global})`);
    }
    const pendingEvents = dbEventlogMissing ? [] : yield* getEventsSince({ global: initialBackendHead, client: EventSequenceNumber_exports.clientDefault });
    const initialSyncState = new SyncState({
      pending: pendingEvents,
      upstreamHead: { global: initialBackendHead, client: EventSequenceNumber_exports.clientDefault },
      localHead: initialLocalHead
    });
    yield* SubscriptionRef_exports.set(syncStateSref, initialSyncState);
    if (pendingEvents.length > 0) {
      const globalPendingEvents = pendingEvents.filter((eventEncoded) => {
        const { eventDef } = getEventDef(schema, eventEncoded.name);
        return eventDef.options.clientOnly === false;
      });
      if (globalPendingEvents.length > 0) {
        yield* BucketQueue_exports.offerAll(syncBackendPushQueue, globalPendingEvents);
      }
    }
    const shutdownOnError = (cause) => Effect_exports.gen(function* () {
      if (onError === "shutdown") {
        yield* shutdownChannel.send(UnexpectedError.make({ cause }));
        yield* Effect_exports.die(cause);
      }
    });
    yield* backgroundApplyLocalPushes({
      localPushesLatch,
      localPushesQueue,
      pullLatch,
      syncStateSref,
      syncBackendPushQueue,
      schema,
      isClientEvent,
      otelSpan,
      currentLocalPushGenerationRef,
      connectedClientSessionPullQueues,
      mergeCounterRef,
      mergePayloads,
      localPushBatchSize,
      testing: {
        delay: (_a = testing == null ? void 0 : testing.delays) == null ? void 0 : _a.localPushProcessing
      }
    }).pipe(Effect_exports.tapCauseLogPretty, Effect_exports.catchAllCause(shutdownOnError), Effect_exports.forkScoped);
    const backendPushingFiberHandle = yield* FiberHandle_exports.make();
    const backendPushingEffect = backgroundBackendPushing({
      syncBackendPushQueue,
      otelSpan,
      devtoolsLatch: (_b = ctxRef.current) == null ? void 0 : _b.devtoolsLatch,
      backendPushBatchSize
    }).pipe(Effect_exports.tapCauseLogPretty, Effect_exports.catchAllCause(shutdownOnError));
    yield* FiberHandle_exports.run(backendPushingFiberHandle, backendPushingEffect);
    yield* backgroundBackendPulling({
      initialBackendHead,
      isClientEvent,
      restartBackendPushing: (filteredRebasedPending) => Effect_exports.gen(function* () {
        yield* FiberHandle_exports.clear(backendPushingFiberHandle);
        yield* BucketQueue_exports.clear(syncBackendPushQueue);
        yield* BucketQueue_exports.offerAll(syncBackendPushQueue, filteredRebasedPending);
        yield* FiberHandle_exports.run(backendPushingFiberHandle, backendPushingEffect);
      }),
      syncStateSref,
      localPushesLatch,
      pullLatch,
      dbState,
      otelSpan,
      initialBlockingSyncContext,
      devtoolsLatch: (_c = ctxRef.current) == null ? void 0 : _c.devtoolsLatch,
      connectedClientSessionPullQueues,
      mergeCounterRef,
      mergePayloads,
      advancePushHead
    }).pipe(Effect_exports.tapCauseLogPretty, Effect_exports.catchAllCause(shutdownOnError), Effect_exports.forkScoped);
    return { initialLeaderHead: initialLocalHead };
  }).pipe(Effect_exports.withSpanScoped("@livestore/common:LeaderSyncProcessor:boot"));
  const pull = ({ cursor }) => Effect_exports.gen(function* () {
    const queue = yield* pullQueue({ cursor });
    return Stream_exports.fromQueue(queue);
  }).pipe(Stream_exports.unwrapScoped);
  const pullQueue = ({ cursor }) => {
    var _a;
    const runtime = ((_a = ctxRef.current) == null ? void 0 : _a.runtime) ?? shouldNeverHappen("Not initialized");
    return Effect_exports.gen(function* () {
      const queue = yield* connectedClientSessionPullQueues.makeQueue;
      const payloadsSinceCursor = Array.from(mergePayloads.entries()).map(([mergeCounter, payload]) => ({ payload, mergeCounter })).filter(({ mergeCounter }) => mergeCounter > cursor.mergeCounter).toSorted((a, b) => a.mergeCounter - b.mergeCounter).map(({ payload, mergeCounter }) => {
        if (payload._tag === "upstream-advance") {
          return {
            payload: {
              _tag: "upstream-advance",
              newEvents: Array_exports.dropWhile(payload.newEvents, (eventEncoded) => EventSequenceNumber_exports.isGreaterThanOrEqual(cursor.eventNum, eventEncoded.seqNum))
            },
            mergeCounter
          };
        } else {
          return { payload, mergeCounter };
        }
      });
      yield* queue.offerAll(payloadsSinceCursor);
      return queue;
    }).pipe(Effect_exports.provide(runtime));
  };
  const syncState = Subscribable_exports.make({
    get: Effect_exports.gen(function* () {
      const syncState2 = yield* syncStateSref;
      if (syncState2 === void 0)
        return shouldNeverHappen("Not initialized");
      return syncState2;
    }),
    changes: syncStateSref.changes.pipe(Stream_exports.filter(isNotUndefined))
  });
  return {
    pull,
    pullQueue,
    push,
    pushPartial,
    boot,
    syncState,
    getMergeCounter: () => mergeCounterRef.current
  };
});
var backgroundApplyLocalPushes = ({ localPushesLatch, localPushesQueue, pullLatch, syncStateSref, syncBackendPushQueue, schema, isClientEvent, otelSpan, currentLocalPushGenerationRef, connectedClientSessionPullQueues, mergeCounterRef, mergePayloads, localPushBatchSize, testing }) => Effect_exports.gen(function* () {
  while (true) {
    if (testing.delay !== void 0) {
      yield* testing.delay.pipe(Effect_exports.withSpan("localPushProcessingDelay"));
    }
    const batchItems = yield* BucketQueue_exports.takeBetween(localPushesQueue, 1, localPushBatchSize);
    yield* localPushesLatch.await;
    yield* pullLatch.close;
    const filteredBatchItems = batchItems.filter(([_1, _2, generation]) => generation === currentLocalPushGenerationRef.current).map(([eventEncoded, deferred]) => [eventEncoded, deferred]);
    if (filteredBatchItems.length === 0) {
      yield* pullLatch.open;
      continue;
    }
    const [newEvents, deferreds] = Array_exports.unzip(filteredBatchItems);
    const syncState = yield* syncStateSref;
    if (syncState === void 0)
      return shouldNeverHappen("Not initialized");
    const mergeResult = merge({
      syncState,
      payload: { _tag: "local-push", newEvents },
      isClientEvent,
      isEqualEvent: LiveStoreEvent_exports.isEqualEncoded
    });
    const mergeCounter = yield* incrementMergeCounter(mergeCounterRef);
    switch (mergeResult._tag) {
      case "unexpected-error": {
        otelSpan == null ? void 0 : otelSpan.addEvent(`[${mergeCounter}]:push:unexpected-error`, {
          batchSize: newEvents.length,
          newEvents: TRACE_VERBOSE ? JSON.stringify(newEvents) : void 0
        });
        return yield* Effect_exports.fail(mergeResult.cause);
      }
      case "rebase": {
        return shouldNeverHappen("The leader thread should never have to rebase due to a local push");
      }
      case "reject": {
        otelSpan == null ? void 0 : otelSpan.addEvent(`[${mergeCounter}]:push:reject`, {
          batchSize: newEvents.length,
          mergeResult: TRACE_VERBOSE ? JSON.stringify(mergeResult) : void 0
        });
        currentLocalPushGenerationRef.current++;
        const nextGeneration = currentLocalPushGenerationRef.current;
        const providedNum = newEvents.at(0).seqNum;
        const remainingEventsMatchingGeneration = yield* BucketQueue_exports.takeSplitWhere(localPushesQueue, (item) => item[2] >= nextGeneration);
        if (LS_DEV && (yield* BucketQueue_exports.size(localPushesQueue)) > 0) {
          console.log("localPushesQueue is not empty", yield* BucketQueue_exports.size(localPushesQueue));
          debugger;
        }
        const allDeferredsToReject = [
          ...deferreds,
          ...remainingEventsMatchingGeneration.map(([_, deferred]) => deferred)
        ].filter(isNotUndefined);
        yield* Effect_exports.forEach(allDeferredsToReject, (deferred) => Deferred_exports.fail(deferred, LeaderAheadError.make({
          minimumExpectedNum: mergeResult.expectedMinimumId,
          providedNum
          // nextGeneration,
        })));
        yield* pullLatch.open;
        continue;
      }
      case "advance": {
        break;
      }
      default: {
        casesHandled(mergeResult);
      }
    }
    yield* SubscriptionRef_exports.set(syncStateSref, mergeResult.newSyncState);
    yield* connectedClientSessionPullQueues.offer({
      payload: PayloadUpstreamAdvance.make({ newEvents: mergeResult.newEvents }),
      mergeCounter
    });
    mergePayloads.set(mergeCounter, PayloadUpstreamAdvance.make({ newEvents: mergeResult.newEvents }));
    otelSpan == null ? void 0 : otelSpan.addEvent(`[${mergeCounter}]:push:advance`, {
      batchSize: newEvents.length,
      mergeResult: TRACE_VERBOSE ? JSON.stringify(mergeResult) : void 0
    });
    const filteredBatch = mergeResult.newEvents.filter((eventEncoded) => {
      const { eventDef } = getEventDef(schema, eventEncoded.name);
      return eventDef.options.clientOnly === false;
    });
    yield* BucketQueue_exports.offerAll(syncBackendPushQueue, filteredBatch);
    yield* materializeEventsBatch({ batchItems: mergeResult.newEvents, deferreds });
    yield* pullLatch.open;
  }
});
var materializeEventsBatch = ({ batchItems, deferreds }) => Effect_exports.gen(function* () {
  const { dbState: db, dbEventlog, materializeEvent } = yield* LeaderThreadCtx;
  db.execute("BEGIN TRANSACTION", void 0);
  dbEventlog.execute("BEGIN TRANSACTION", void 0);
  yield* Effect_exports.addFinalizer((exit) => Effect_exports.gen(function* () {
    if (Exit_exports.isSuccess(exit))
      return;
    db.execute("ROLLBACK", void 0);
    dbEventlog.execute("ROLLBACK", void 0);
  }));
  for (let i = 0; i < batchItems.length; i++) {
    const { sessionChangeset, hash } = yield* materializeEvent(batchItems[i]);
    batchItems[i].meta.sessionChangeset = sessionChangeset;
    batchItems[i].meta.materializerHashLeader = hash;
    if ((deferreds == null ? void 0 : deferreds[i]) !== void 0) {
      yield* Deferred_exports.succeed(deferreds[i], void 0);
    }
  }
  db.execute("COMMIT", void 0);
  dbEventlog.execute("COMMIT", void 0);
}).pipe(Effect_exports.uninterruptible, Effect_exports.scoped, Effect_exports.withSpan("@livestore/common:LeaderSyncProcessor:materializeEventItems", {
  attributes: { batchSize: batchItems.length }
}), Effect_exports.tapCauseLogPretty, UnexpectedError.mapToUnexpectedError);
var backgroundBackendPulling = ({ initialBackendHead, isClientEvent, restartBackendPushing, otelSpan, dbState, syncStateSref, localPushesLatch, pullLatch, devtoolsLatch, initialBlockingSyncContext, connectedClientSessionPullQueues, mergeCounterRef, mergePayloads, advancePushHead }) => Effect_exports.gen(function* () {
  const { syncBackend, dbState: db, dbEventlog, schema } = yield* LeaderThreadCtx;
  if (syncBackend === void 0)
    return;
  const onNewPullChunk = (newEvents, remaining) => Effect_exports.gen(function* () {
    if (newEvents.length === 0)
      return;
    if (devtoolsLatch !== void 0) {
      yield* devtoolsLatch.await;
    }
    yield* localPushesLatch.close;
    yield* pullLatch.await;
    const syncState = yield* syncStateSref;
    if (syncState === void 0)
      return shouldNeverHappen("Not initialized");
    const mergeResult = merge({
      syncState,
      payload: PayloadUpstreamAdvance.make({ newEvents }),
      isClientEvent,
      isEqualEvent: LiveStoreEvent_exports.isEqualEncoded,
      ignoreClientEvents: true
    });
    const mergeCounter = yield* incrementMergeCounter(mergeCounterRef);
    if (mergeResult._tag === "reject") {
      return shouldNeverHappen("The leader thread should never reject upstream advances");
    } else if (mergeResult._tag === "unexpected-error") {
      otelSpan == null ? void 0 : otelSpan.addEvent(`[${mergeCounter}]:pull:unexpected-error`, {
        newEventsCount: newEvents.length,
        newEvents: TRACE_VERBOSE ? JSON.stringify(newEvents) : void 0
      });
      return yield* Effect_exports.fail(mergeResult.cause);
    }
    const newBackendHead = newEvents.at(-1).seqNum;
    updateBackendHead(dbEventlog, newBackendHead);
    if (mergeResult._tag === "rebase") {
      otelSpan == null ? void 0 : otelSpan.addEvent(`[${mergeCounter}]:pull:rebase`, {
        newEventsCount: newEvents.length,
        newEvents: TRACE_VERBOSE ? JSON.stringify(newEvents) : void 0,
        rollbackCount: mergeResult.rollbackEvents.length,
        mergeResult: TRACE_VERBOSE ? JSON.stringify(mergeResult) : void 0
      });
      const globalRebasedPendingEvents = mergeResult.newSyncState.pending.filter((event) => {
        const { eventDef } = getEventDef(schema, event.name);
        return eventDef.options.clientOnly === false;
      });
      yield* restartBackendPushing(globalRebasedPendingEvents);
      if (mergeResult.rollbackEvents.length > 0) {
        yield* rollback({
          dbState: db,
          dbEventlog,
          eventNumsToRollback: mergeResult.rollbackEvents.map((_) => _.seqNum)
        });
      }
      yield* connectedClientSessionPullQueues.offer({
        payload: PayloadUpstreamRebase.make({
          newEvents: mergeResult.newEvents,
          rollbackEvents: mergeResult.rollbackEvents
        }),
        mergeCounter
      });
      mergePayloads.set(mergeCounter, PayloadUpstreamRebase.make({
        newEvents: mergeResult.newEvents,
        rollbackEvents: mergeResult.rollbackEvents
      }));
    } else {
      otelSpan == null ? void 0 : otelSpan.addEvent(`[${mergeCounter}]:pull:advance`, {
        newEventsCount: newEvents.length,
        mergeResult: TRACE_VERBOSE ? JSON.stringify(mergeResult) : void 0
      });
      yield* connectedClientSessionPullQueues.offer({
        payload: PayloadUpstreamAdvance.make({ newEvents: mergeResult.newEvents }),
        mergeCounter
      });
      mergePayloads.set(mergeCounter, PayloadUpstreamAdvance.make({ newEvents: mergeResult.newEvents }));
      if (mergeResult.confirmedEvents.length > 0) {
        const confirmedNewEvents = newEvents.filter((event) => mergeResult.confirmedEvents.some((confirmedEvent) => EventSequenceNumber_exports.isEqual(event.seqNum, confirmedEvent.seqNum)));
        yield* updateSyncMetadata(confirmedNewEvents);
      }
    }
    trimChangesetRows(db, newBackendHead);
    advancePushHead(mergeResult.newSyncState.localHead);
    yield* materializeEventsBatch({ batchItems: mergeResult.newEvents, deferreds: void 0 });
    yield* SubscriptionRef_exports.set(syncStateSref, mergeResult.newSyncState);
    if (remaining === 0) {
      yield* localPushesLatch.open;
    }
  });
  const cursorInfo = yield* getSyncBackendCursorInfo(initialBackendHead);
  const hashMaterializerResult = makeMaterializerHash({ schema, dbState });
  yield* syncBackend.pull(cursorInfo).pipe(
    // TODO only take from queue while connected
    Stream_exports.tap(({ batch, remaining }) => Effect_exports.gen(function* () {
      yield* SubscriptionRef_exports.waitUntil(syncBackend.isConnected, (isConnected) => isConnected === true);
      yield* onNewPullChunk(batch.map((_) => LiveStoreEvent_exports.EncodedWithMeta.fromGlobal(_.eventEncoded, {
        syncMetadata: _.metadata,
        materializerHashLeader: hashMaterializerResult(_.eventEncoded),
        materializerHashSession: Option_exports.none()
      })), remaining);
      yield* initialBlockingSyncContext.update({ processed: batch.length, remaining });
    })),
    Stream_exports.runDrain,
    Effect_exports.interruptible
  );
}).pipe(Effect_exports.withSpan("@livestore/common:LeaderSyncProcessor:backend-pulling"));
var backgroundBackendPushing = ({ syncBackendPushQueue, otelSpan, devtoolsLatch, backendPushBatchSize }) => Effect_exports.gen(function* () {
  const { syncBackend } = yield* LeaderThreadCtx;
  if (syncBackend === void 0)
    return;
  while (true) {
    yield* SubscriptionRef_exports.waitUntil(syncBackend.isConnected, (isConnected) => isConnected === true);
    const queueItems = yield* BucketQueue_exports.takeBetween(syncBackendPushQueue, 1, backendPushBatchSize);
    yield* SubscriptionRef_exports.waitUntil(syncBackend.isConnected, (isConnected) => isConnected === true);
    if (devtoolsLatch !== void 0) {
      yield* devtoolsLatch.await;
    }
    otelSpan == null ? void 0 : otelSpan.addEvent("backend-push", {
      batchSize: queueItems.length,
      batch: TRACE_VERBOSE ? JSON.stringify(queueItems) : void 0
    });
    const pushResult = yield* syncBackend.push(queueItems.map((_) => _.toGlobal())).pipe(Effect_exports.either);
    if (pushResult._tag === "Left") {
      if (LS_DEV) {
        yield* Effect_exports.logDebug("handled backend-push-error", { error: pushResult.left.toString() });
      }
      otelSpan == null ? void 0 : otelSpan.addEvent("backend-push-error", { error: pushResult.left.toString() });
      return yield* Effect_exports.never;
    }
  }
}).pipe(Effect_exports.interruptible, Effect_exports.withSpan("@livestore/common:LeaderSyncProcessor:backend-pushing"));
var trimChangesetRows = (db, newHead) => {
  db.execute(sql`DELETE FROM ${system_tables_exports.SESSION_CHANGESET_META_TABLE} WHERE seqNumGlobal < ${newHead.global}`);
};
var makePullQueueSet = Effect_exports.gen(function* () {
  const set = /* @__PURE__ */ new Set();
  yield* Effect_exports.addFinalizer(() => Effect_exports.gen(function* () {
    for (const queue of set) {
      yield* Queue_exports.shutdown(queue);
    }
    set.clear();
  }));
  const makeQueue = Effect_exports.gen(function* () {
    const queue = yield* Queue_exports.unbounded().pipe(Effect_exports.acquireRelease(Queue_exports.shutdown));
    yield* Effect_exports.addFinalizer(() => Effect_exports.sync(() => set.delete(queue)));
    set.add(queue);
    return queue;
  });
  const offer = (item) => Effect_exports.gen(function* () {
    if (item.payload._tag === "upstream-advance" && item.payload.newEvents.length === 0) {
      return;
    }
    for (const queue of set) {
      yield* Queue_exports.offer(queue, item);
    }
  });
  return {
    makeQueue,
    offer
  };
});
var incrementMergeCounter = (mergeCounterRef) => Effect_exports.gen(function* () {
  const { dbState } = yield* LeaderThreadCtx;
  mergeCounterRef.current++;
  dbState.execute(sql`INSERT OR REPLACE INTO ${system_tables_exports.LEADER_MERGE_COUNTER_TABLE} (id, mergeCounter) VALUES (0, ${mergeCounterRef.current})`);
  return mergeCounterRef.current;
});
var getMergeCounterFromDb = (dbState) => Effect_exports.gen(function* () {
  var _a;
  const result = dbState.select(sql`SELECT mergeCounter FROM ${system_tables_exports.LEADER_MERGE_COUNTER_TABLE} WHERE id = 0`);
  return ((_a = result[0]) == null ? void 0 : _a.mergeCounter) ?? 0;
});
var validatePushBatch = (batch, pushHead) => Effect_exports.gen(function* () {
  if (batch.length === 0) {
    return;
  }
  for (let i = 1; i < batch.length; i++) {
    if (EventSequenceNumber_exports.isGreaterThanOrEqual(batch[i - 1].seqNum, batch[i].seqNum)) {
      shouldNeverHappen(`Events must be ordered in monotonically ascending order by eventNum. Received: [${batch.map((e) => EventSequenceNumber_exports.toString(e.seqNum)).join(", ")}]`);
    }
  }
  if (EventSequenceNumber_exports.isGreaterThanOrEqual(pushHead, batch[0].seqNum)) {
    return yield* LeaderAheadError.make({
      minimumExpectedNum: pushHead,
      providedNum: batch[0].seqNum
    });
  }
});

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/recreate-db.js
var recreateDb = Effect_exports.gen(function* () {
  const { dbState, dbEventlog, schema, bootStatusQueue, materializeEvent } = yield* LeaderThreadCtx;
  const migrationOptions = schema.state.sqlite.migrations;
  let migrationsReport;
  yield* Effect_exports.addFinalizer(Effect_exports.fn("recreateDb:finalizer")(function* (ex) {
    if (ex._tag === "Failure")
      dbState.destroy();
  }));
  const tmpDb = dbState;
  yield* configureConnection(tmpDb, { foreignKeys: true });
  const initDb = (hooks) => Effect_exports.gen(function* () {
    yield* Effect_exports.tryAll(() => {
      var _a;
      return (_a = hooks == null ? void 0 : hooks.init) == null ? void 0 : _a.call(hooks, tmpDb);
    }).pipe(UnexpectedError.mapToUnexpectedError);
    const migrationsReport2 = yield* migrateDb({
      db: tmpDb,
      schema,
      onProgress: ({ done, total }) => Queue_exports.offer(bootStatusQueue, { stage: "migrating", progress: { done, total } })
    });
    yield* Effect_exports.tryAll(() => {
      var _a;
      return (_a = hooks == null ? void 0 : hooks.pre) == null ? void 0 : _a.call(hooks, tmpDb);
    }).pipe(UnexpectedError.mapToUnexpectedError);
    return { migrationsReport: migrationsReport2, tmpDb };
  });
  switch (migrationOptions.strategy) {
    case "auto": {
      const hooks = migrationOptions.hooks;
      const initResult = yield* initDb(hooks);
      migrationsReport = initResult.migrationsReport;
      yield* rematerializeFromEventlog({
        // db: initResult.tmpDb,
        dbEventlog,
        schema,
        materializeEvent,
        onProgress: ({ done, total }) => Queue_exports.offer(bootStatusQueue, { stage: "rehydrating", progress: { done, total } })
      });
      yield* Effect_exports.tryAll(() => {
        var _a;
        return (_a = hooks == null ? void 0 : hooks.post) == null ? void 0 : _a.call(hooks, initResult.tmpDb);
      }).pipe(UnexpectedError.mapToUnexpectedError);
      break;
    }
    case "manual": {
      const oldDbData = dbState.export();
      migrationsReport = { migrations: [] };
      const newDbData = yield* Effect_exports.tryAll(() => migrationOptions.migrate(oldDbData)).pipe(UnexpectedError.mapToUnexpectedError);
      tmpDb.import(newDbData);
      break;
    }
    default: {
      casesHandled(migrationOptions);
    }
  }
  return { migrationsReport };
}).pipe(
  Effect_exports.scoped,
  // NOTE we're closing the scope here so finalizers are called when the effect is done
  Effect_exports.withSpan("@livestore/common:leader-thread:recreateDb"),
  Effect_exports.withPerformanceMeasure("@livestore/common:leader-thread:recreateDb")
);

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/make-leader-thread-layer.js
var makeLeaderThreadLayer = ({ schema, storeId, clientId, syncPayload, makeSqliteDb: makeSqliteDb2, syncOptions, dbState, dbEventlog, devtoolsOptions, shutdownChannel, params, testing }) => Effect_exports.gen(function* () {
  var _a;
  const bootStatusQueue = yield* Queue_exports.unbounded().pipe(Effect_exports.acquireRelease(Queue_exports.shutdown));
  const dbEventlogMissing = dbEventlog.select(sql`select count(*) as count from sqlite_master`)[0].count === 0;
  const dbStateMissing = dbState.select(sql`select count(*) as count from sqlite_master`)[0].count === 0;
  const syncBackend = (syncOptions == null ? void 0 : syncOptions.backend) === void 0 ? void 0 : yield* syncOptions.backend({ storeId, clientId, payload: syncPayload });
  if (syncBackend !== void 0) {
    yield* syncBackend.connect.pipe(Effect_exports.tapCauseLogPretty, Effect_exports.forkScoped);
  }
  const initialBlockingSyncContext = yield* makeInitialBlockingSyncContext({
    initialSyncOptions: (syncOptions == null ? void 0 : syncOptions.initialSyncOptions) ?? { _tag: "Skip" },
    bootStatusQueue
  });
  const syncProcessor = yield* makeLeaderSyncProcessor({
    schema,
    dbEventlogMissing,
    dbEventlog,
    dbState,
    dbStateMissing,
    initialBlockingSyncContext,
    onError: (syncOptions == null ? void 0 : syncOptions.onSyncError) ?? "ignore",
    params: {
      localPushBatchSize: params == null ? void 0 : params.localPushBatchSize,
      backendPushBatchSize: params == null ? void 0 : params.backendPushBatchSize
    },
    testing: {
      delays: (_a = testing == null ? void 0 : testing.syncProcessor) == null ? void 0 : _a.delays
    }
  });
  const extraIncomingMessagesQueue = yield* Queue_exports.unbounded().pipe(Effect_exports.acquireRelease(Queue_exports.shutdown));
  const devtoolsContext = devtoolsOptions.enabled ? {
    enabled: true,
    syncBackendLatch: yield* Effect_exports.makeLatch(true),
    syncBackendLatchState: yield* SubscriptionRef_exports.make({ latchClosed: false })
  } : { enabled: false };
  const materializeEvent = yield* makeMaterializeEvent({ schema, dbState, dbEventlog });
  const ctx = {
    schema,
    bootStatusQueue,
    storeId,
    clientId,
    dbState,
    dbEventlog,
    makeSqliteDb: makeSqliteDb2,
    eventSchema: LiveStoreEvent_exports.makeEventDefSchema(schema),
    shutdownStateSubRef: yield* SubscriptionRef_exports.make("running"),
    shutdownChannel,
    syncBackend,
    syncProcessor,
    materializeEvent,
    extraIncomingMessagesQueue,
    devtools: devtoolsContext,
    // State will be set during `bootLeaderThread`
    initialState: {}
  };
  globalThis.__leaderThreadCtx = ctx;
  const layer = Layer_exports.succeed(LeaderThreadCtx, ctx);
  ctx.initialState = yield* bootLeaderThread({
    dbStateMissing,
    initialBlockingSyncContext,
    devtoolsOptions
  }).pipe(Effect_exports.provide(layer));
  return layer;
}).pipe(Effect_exports.withSpan("@livestore/common:leader-thread:boot"), Effect_exports.withSpanScoped("@livestore/common:leader-thread"), UnexpectedError.mapToUnexpectedError, Effect_exports.tapCauseLogPretty, Layer_exports.unwrapScoped);
var makeInitialBlockingSyncContext = ({ initialSyncOptions, bootStatusQueue }) => Effect_exports.gen(function* () {
  const ctx = {
    isDone: false,
    processedEvents: 0,
    total: -1
  };
  const blockingDeferred = initialSyncOptions._tag === "Blocking" ? yield* Deferred_exports.make() : void 0;
  if (blockingDeferred !== void 0 && initialSyncOptions._tag === "Blocking") {
    yield* Deferred_exports.succeed(blockingDeferred, void 0).pipe(Effect_exports.delay(initialSyncOptions.timeout), Effect_exports.forkScoped);
  }
  return {
    blockingDeferred,
    update: ({ processed, remaining }) => Effect_exports.gen(function* () {
      if (ctx.isDone === true)
        return;
      if (ctx.total === -1) {
        ctx.total = remaining + processed;
      }
      ctx.processedEvents += processed;
      yield* Queue_exports.offer(bootStatusQueue, {
        stage: "syncing",
        progress: { done: ctx.processedEvents, total: ctx.total }
      });
      if (remaining === 0 && blockingDeferred !== void 0) {
        yield* Deferred_exports.succeed(blockingDeferred, void 0);
        ctx.isDone = true;
      }
    })
  };
});
var bootLeaderThread = ({ dbStateMissing, initialBlockingSyncContext, devtoolsOptions }) => Effect_exports.gen(function* () {
  const { dbEventlog, bootStatusQueue, syncProcessor } = yield* LeaderThreadCtx;
  yield* initEventlogDb(dbEventlog);
  const { migrationsReport } = dbStateMissing ? yield* recreateDb : { migrationsReport: { migrations: [] } };
  const { initialLeaderHead } = yield* syncProcessor.boot;
  if (initialBlockingSyncContext.blockingDeferred !== void 0) {
    yield* Queue_exports.offer(bootStatusQueue, {
      stage: "syncing",
      progress: { done: 0, total: -1 }
    });
    yield* initialBlockingSyncContext.blockingDeferred.pipe(Effect_exports.withSpan("@livestore/common:leader-thread:initial-sync-blocking"));
  }
  yield* Queue_exports.offer(bootStatusQueue, { stage: "done" });
  yield* bootDevtools(devtoolsOptions).pipe(Effect_exports.tapCauseLogPretty, Effect_exports.forkScoped);
  return { migrationsReport, leaderHead: initialLeaderHead };
});

// node_modules/.pnpm/@livestore+common@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/common/dist/leader-thread/shutdown-channel.js
var shutdown_channel_exports = {};
__export(shutdown_channel_exports, {
  All: () => All
});
var All = class extends Schema_exports.Union(IntentionalShutdownCause, UnexpectedError) {
};

// node_modules/.pnpm/@livestore+devtools-web-common@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/devtools-web-common/dist/worker/schema.js
var schema_exports = {};
__export(schema_exports, {
  CreateConnection: () => CreateConnection,
  Request: () => Request
});
var CreateConnection = class extends Schema_exports.TaggedRequest()("DevtoolsWebCommon.CreateConnection", {
  payload: {
    from: Schema_exports.String,
    port: Transferable_exports.MessagePort
  },
  success: Schema_exports.Struct({}),
  failure: Schema_exports.Never
}) {
};
var Request = class extends Schema_exports.Union(CreateConnection) {
};

// node_modules/.pnpm/@livestore+devtools-web-common@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/devtools-web-common/dist/worker/mod.js
var _CacheService = class _CacheService extends Context_exports.Tag("@livestore/devtools-web-common:CacheService")() {
};
__publicField(_CacheService, "layer", ({ nodeName }) => Effect_exports.gen(function* () {
  const node = yield* makeMeshNode(nodeName);
  globalThis.__debugWebmeshNode = node;
  return { node };
}).pipe(Layer_exports.scoped(_CacheService)));
var CacheService = _CacheService;
var CreateConnection2 = ({ from, port }) => Stream_exports.asyncScoped((emit) => Effect_exports.gen(function* () {
  const { node } = yield* CacheService;
  const messagePortChannel = yield* mod_exports.messagePortChannel({ port, schema: mesh_schema_exports.Packet });
  yield* node.addEdge({ target: from, edgeChannel: messagePortChannel, replaceIfExists: true });
  if (LS_DEV) {
    yield* Effect_exports.logDebug(`@livestore/devtools-web-common: accepted edge: ${node.nodeName} ← ${from}`);
  }
  emit.single({});
  yield* Effect_exports.spanEvent({ connectedTo: [...node.edgeKeys] });
}).pipe(Effect_exports.orDie)).pipe(Stream_exports.withSpan(`@livestore/devtools-web-common:worker:create-connection:${from}`));

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/in-memory-vfs.js
import { MemoryVFS } from "@livestore/wa-sqlite/src/examples/MemoryVFS.js";
var cachedMemoryVfs;
var makeInMemoryDb = (sqlite3) => {
  if (sqlite3.vfs_registered.has("memory-vfs") === false) {
    const vfs2 = new MemoryVFS("memory-vfs", sqlite3.module);
    sqlite3.vfs_register(vfs2, false);
    cachedMemoryVfs = vfs2;
  }
  const dbPointer = sqlite3.open_v2Sync(":memory:", void 0, "memory-vfs");
  const vfs = cachedMemoryVfs;
  return { dbPointer, vfs };
};

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/make-sqlite-db.js
import * as SqliteConstants from "@livestore/wa-sqlite/src/sqlite-constants.js";
var makeSqliteDb = ({ sqlite3, metadata }) => {
  const preparedStmts = [];
  const { dbPointer } = metadata;
  let isClosed = false;
  const sqliteDb = {
    _tag: "SqliteDb",
    metadata,
    prepare: (queryStr) => {
      try {
        const stmts = sqlite3.statements(dbPointer, queryStr.trim(), { unscoped: true });
        let isFinalized = false;
        const preparedStmt = {
          execute: (bindValues, options) => {
            for (const stmt of stmts) {
              if (bindValues !== void 0 && Object.keys(bindValues).length > 0) {
                sqlite3.bind_collection(stmt, bindValues);
              }
              try {
                sqlite3.step(stmt);
              } finally {
                if (options == null ? void 0 : options.onRowsChanged) {
                  options.onRowsChanged(sqlite3.changes(dbPointer));
                }
                sqlite3.reset(stmt);
              }
            }
          },
          select: (bindValues) => {
            if (stmts.length !== 1) {
              throw new SqliteError({
                query: { bindValues, sql: queryStr },
                code: -1,
                cause: "Expected only one statement when using `select`"
              });
            }
            const stmt = stmts[0];
            if (bindValues !== void 0 && Object.keys(bindValues).length > 0) {
              sqlite3.bind_collection(stmt, bindValues);
            }
            const results = [];
            try {
              let columns = void 0;
              try {
                columns = sqlite3.column_names(stmt);
              } catch (_e) {
              }
              while (sqlite3.step(stmt) === SqliteConstants.SQLITE_ROW) {
                if (columns !== void 0) {
                  const obj = {};
                  for (let i = 0; i < columns.length; i++) {
                    obj[columns[i]] = sqlite3.column(stmt, i);
                  }
                  results.push(obj);
                }
              }
            } catch (e) {
              throw new SqliteError({
                query: { bindValues, sql: queryStr },
                code: e.code,
                cause: e
              });
            } finally {
              sqlite3.reset(stmt);
            }
            return results;
          },
          finalize: () => {
            if (isFinalized) {
              return;
            }
            isFinalized = true;
            for (const stmt of stmts) {
              sqlite3.finalize(stmt);
            }
          },
          sql: queryStr
        };
        preparedStmts.push(preparedStmt);
        return preparedStmt;
      } catch (e) {
        throw new SqliteError({
          query: { sql: queryStr, bindValues: {} },
          code: e.code,
          cause: e
        });
      }
    },
    export: () => sqlite3.serialize(dbPointer, "main"),
    execute: sqlite_db_helper_exports.makeExecute((queryStr, bindValues, options) => {
      const stmt = sqliteDb.prepare(queryStr);
      stmt.execute(bindValues, options);
      stmt.finalize();
    }),
    select: sqlite_db_helper_exports.makeSelect((queryStr, bindValues) => {
      const stmt = sqliteDb.prepare(queryStr);
      const results = stmt.select(bindValues);
      stmt.finalize();
      return results;
    }),
    destroy: () => {
      sqliteDb.close();
      metadata.deleteDb();
    },
    close: () => {
      if (isClosed) {
        return;
      }
      for (const stmt of preparedStmts) {
        stmt.finalize();
      }
      sqlite3.close(dbPointer);
      isClosed = true;
    },
    import: (source) => {
      const FREE_ON_CLOSE = 1;
      const RESIZEABLE = 2;
      if (source instanceof Uint8Array) {
        const tmpDb = makeInMemoryDb(sqlite3);
        sqlite3.deserialize(tmpDb.dbPointer, "main", source, source.length, source.length, FREE_ON_CLOSE | RESIZEABLE);
        sqlite3.backup(dbPointer, "main", tmpDb.dbPointer, "main");
        sqlite3.close(tmpDb.dbPointer);
      } else {
        sqlite3.backup(dbPointer, "main", source.metadata.dbPointer, "main");
      }
      metadata.configureDb(sqliteDb);
    },
    session: () => {
      const sessionPointer = sqlite3.session_create(dbPointer, "main");
      sqlite3.session_attach(sessionPointer, null);
      return {
        changeset: () => {
          const res = sqlite3.session_changeset(sessionPointer);
          return res.changeset ?? void 0;
        },
        finish: () => {
          sqlite3.session_delete(sessionPointer);
        }
      };
    },
    makeChangeset: (data) => {
      const changeset = {
        invert: () => {
          const inverted = sqlite3.changeset_invert(data);
          return sqliteDb.makeChangeset(inverted);
        },
        apply: () => {
          try {
            sqlite3.changeset_apply(dbPointer, data);
            data = void 0;
          } catch (cause) {
            throw new SqliteError({
              code: cause.code ?? -1,
              cause,
              note: `Failed calling makeChangeset.apply`
            });
          }
        }
      };
      return changeset;
    }
  };
  metadata.configureDb(sqliteDb);
  return sqliteDb;
};

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/browser/opfs/AccessHandlePoolVFS.js
import * as VFS2 from "@livestore/wa-sqlite/src/VFS.js";

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/FacadeVFS.js
import * as VFS from "@livestore/wa-sqlite/src/VFS.js";
var AsyncFunction = Object.getPrototypeOf(async () => {
}).constructor;
var _FacadeVFS_instances, makeTypedDataView_fn, makeDataArray_fn, decodeFilename_fn;
var FacadeVFS = class extends VFS.Base {
  /**
   * @param {string} name
   * @param {object} module
   */
  constructor(name, module) {
    super(name, module);
    __privateAdd(this, _FacadeVFS_instances);
  }
  /**
   * Override to indicate which methods are asynchronous.
   * @param {string} methodName
   * @returns {boolean}
   */
  hasAsyncMethod(methodName) {
    const jMethodName = `j${methodName.slice(1)}`;
    return this[jMethodName] instanceof AsyncFunction;
  }
  /**
   * Return the filename for a file id for use by mixins.
   * @param {number} pFile
   * @returns {string}
   */
  getFilename(pFile) {
    throw new Error("unimplemented");
  }
  /**
   * @param {string?} filename
   * @param {number} pFile
   * @param {number} flags
   * @param {DataView} pOutFlags
   * @returns {number|Promise<number>}
   */
  jOpen(filename, pFile, flags, pOutFlags) {
    return VFS.SQLITE_CANTOPEN;
  }
  /**
   * @param {string} filename
   * @param {number} syncDir
   * @returns {number|Promise<number>}
   */
  jDelete(filename, syncDir) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {string} filename
   * @param {number} flags
   * @param {DataView} pResOut
   * @returns {number|Promise<number>}
   */
  jAccess(filename, flags, pResOut) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {string} filename
   * @param {Uint8Array} zOut
   * @returns {number|Promise<number>}
   */
  jFullPathname(filename, zOut) {
    const { read, written } = new TextEncoder().encodeInto(filename, zOut);
    if (read < filename.length)
      return VFS.SQLITE_IOERR;
    if (written >= zOut.length)
      return VFS.SQLITE_IOERR;
    zOut[written] = 0;
    return VFS.SQLITE_OK;
  }
  /**
   * @param {Uint8Array} zBuf
   * @returns {number|Promise<number>}
   */
  jGetLastError(zBuf) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @returns {number|Promise<number>}
   */
  jClose(pFile) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @param {Uint8Array} pData
   * @param {number} iOffset
   * @returns {number|Promise<number>}
   */
  jRead(pFile, pData, iOffset) {
    pData.fill(0);
    return VFS.SQLITE_IOERR_SHORT_READ;
  }
  /**
   * @param {number} pFile
   * @param {Uint8Array} pData
   * @param {number} iOffset
   * @returns {number|Promise<number>}
   */
  jWrite(pFile, pData, iOffset) {
    return VFS.SQLITE_IOERR_WRITE;
  }
  /**
   * @param {number} pFile
   * @param {number} size
   * @returns {number|Promise<number>}
   */
  jTruncate(pFile, size) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @param {number} flags
   * @returns {number|Promise<number>}
   */
  jSync(pFile, flags) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @param {DataView} pSize
   * @returns {number|Promise<number>}
   */
  jFileSize(pFile, pSize) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @param {number} lockType
   * @returns {number|Promise<number>}
   */
  jLock(pFile, lockType) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @param {number} lockType
   * @returns {number|Promise<number>}
   */
  jUnlock(pFile, lockType) {
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @param {DataView} pResOut
   * @returns {number|Promise<number>}
   */
  jCheckReservedLock(pFile, pResOut) {
    pResOut.setInt32(0, 0, true);
    return VFS.SQLITE_OK;
  }
  /**
   * @param {number} pFile
   * @param {number} op
   * @param {DataView} pArg
   * @returns {number|Promise<number>}
   */
  jFileControl(pFile, op, pArg) {
    return VFS.SQLITE_NOTFOUND;
  }
  /**
   * @param {number} pFile
   * @returns {number|Promise<number>}
   */
  jSectorSize(pFile) {
    return super.xSectorSize(pFile);
  }
  /**
   * @param {number} pFile
   * @returns {number|Promise<number>}
   */
  jDeviceCharacteristics(pFile) {
    return 0;
  }
  /**
   * @param {number} pVfs
   * @param {number} zName
   * @param {number} pFile
   * @param {number} flags
   * @param {number} pOutFlags
   * @returns {number|Promise<number>}
   */
  xOpen(pVfs, zName, pFile, flags, pOutFlags) {
    var _a;
    const filename = __privateMethod(this, _FacadeVFS_instances, decodeFilename_fn).call(this, zName, flags);
    const pOutFlagsView = __privateMethod(this, _FacadeVFS_instances, makeTypedDataView_fn).call(this, "Int32", pOutFlags);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jOpen", filename, pFile, "0x" + flags.toString(16));
    return this.jOpen(filename, pFile, flags, pOutFlagsView);
  }
  /**
   * @param {number} pVfs
   * @param {number} zName
   * @param {number} syncDir
   * @returns {number|Promise<number>}
   */
  xDelete(pVfs, zName, syncDir) {
    var _a;
    const filename = this._module.UTF8ToString(zName);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jDelete", filename, syncDir);
    return this.jDelete(filename, syncDir);
  }
  /**
   * @param {number} pVfs
   * @param {number} zName
   * @param {number} flags
   * @param {number} pResOut
   * @returns {number|Promise<number>}
   */
  xAccess(pVfs, zName, flags, pResOut) {
    var _a;
    const filename = this._module.UTF8ToString(zName);
    const pResOutView = __privateMethod(this, _FacadeVFS_instances, makeTypedDataView_fn).call(this, "Int32", pResOut);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jAccess", filename, flags);
    return this.jAccess(filename, flags, pResOutView);
  }
  /**
   * @param {number} pVfs
   * @param {number} zName
   * @param {number} nOut
   * @param {number} zOut
   * @returns {number|Promise<number>}
   */
  xFullPathname(pVfs, zName, nOut, zOut) {
    var _a;
    const filename = this._module.UTF8ToString(zName);
    const zOutArray = this._module.HEAPU8.subarray(zOut, zOut + nOut);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jFullPathname", filename, nOut);
    return this.jFullPathname(filename, zOutArray);
  }
  /**
   * @param {number} pVfs
   * @param {number} nBuf
   * @param {number} zBuf
   * @returns {number|Promise<number>}
   */
  xGetLastError(pVfs, nBuf, zBuf) {
    var _a;
    const zBufArray = this._module.HEAPU8.subarray(zBuf, zBuf + nBuf);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jGetLastError", nBuf);
    return this.jGetLastError(zBufArray);
  }
  /**
   * @param {number} pFile
   * @returns {number|Promise<number>}
   */
  xClose(pFile) {
    var _a;
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jClose", pFile);
    return this.jClose(pFile);
  }
  /**
   * @param {number} pFile
   * @param {number} pData
   * @param {number} iAmt
   * @param {number} iOffsetLo
   * @param {number} iOffsetHi
   * @returns {number|Promise<number>}
   */
  xRead(pFile, pData, iAmt, iOffsetLo, iOffsetHi) {
    var _a;
    const pDataArray = __privateMethod(this, _FacadeVFS_instances, makeDataArray_fn).call(this, pData, iAmt);
    const iOffset = delegalize(iOffsetLo, iOffsetHi);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jRead", pFile, iAmt, iOffset);
    return this.jRead(pFile, pDataArray, iOffset);
  }
  /**
   * @param {number} pFile
   * @param {number} pData
   * @param {number} iAmt
   * @param {number} iOffsetLo
   * @param {number} iOffsetHi
   * @returns {number|Promise<number>}
   */
  xWrite(pFile, pData, iAmt, iOffsetLo, iOffsetHi) {
    var _a;
    const pDataArray = __privateMethod(this, _FacadeVFS_instances, makeDataArray_fn).call(this, pData, iAmt);
    const iOffset = delegalize(iOffsetLo, iOffsetHi);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jWrite", pFile, pDataArray, iOffset);
    return this.jWrite(pFile, pDataArray, iOffset);
  }
  /**
   * @param {number} pFile
   * @param {number} sizeLo
   * @param {number} sizeHi
   * @returns {number|Promise<number>}
   */
  xTruncate(pFile, sizeLo, sizeHi) {
    var _a;
    const size = delegalize(sizeLo, sizeHi);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jTruncate", pFile, size);
    return this.jTruncate(pFile, size);
  }
  /**
   * @param {number} pFile
   * @param {number} flags
   * @returns {number|Promise<number>}
   */
  xSync(pFile, flags) {
    var _a;
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jSync", pFile, flags);
    return this.jSync(pFile, flags);
  }
  /**
   *
   * @param {number} pFile
   * @param {number} pSize
   * @returns {number|Promise<number>}
   */
  xFileSize(pFile, pSize) {
    var _a;
    const pSizeView = __privateMethod(this, _FacadeVFS_instances, makeTypedDataView_fn).call(this, "BigInt64", pSize);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jFileSize", pFile);
    return this.jFileSize(pFile, pSizeView);
  }
  /**
   * @param {number} pFile
   * @param {number} lockType
   * @returns {number|Promise<number>}
   */
  xLock(pFile, lockType) {
    var _a;
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jLock", pFile, lockType);
    return this.jLock(pFile, lockType);
  }
  /**
   * @param {number} pFile
   * @param {number} lockType
   * @returns {number|Promise<number>}
   */
  xUnlock(pFile, lockType) {
    var _a;
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jUnlock", pFile, lockType);
    return this.jUnlock(pFile, lockType);
  }
  /**
   * @param {number} pFile
   * @param {number} pResOut
   * @returns {number|Promise<number>}
   */
  xCheckReservedLock(pFile, pResOut) {
    var _a;
    const pResOutView = __privateMethod(this, _FacadeVFS_instances, makeTypedDataView_fn).call(this, "Int32", pResOut);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jCheckReservedLock", pFile);
    return this.jCheckReservedLock(pFile, pResOutView);
  }
  /**
   * @param {number} pFile
   * @param {number} op
   * @param {number} pArg
   * @returns {number|Promise<number>}
   */
  xFileControl(pFile, op, pArg) {
    var _a;
    const pArgView = new DataView(this._module.HEAPU8.buffer, this._module.HEAPU8.byteOffset + pArg);
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jFileControl", pFile, op, pArgView);
    return this.jFileControl(pFile, op, pArgView);
  }
  /**
   * @param {number} pFile
   * @returns {number|Promise<number>}
   */
  xSectorSize(pFile) {
    var _a;
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jSectorSize", pFile);
    return this.jSectorSize(pFile);
  }
  /**
   * @param {number} pFile
   * @returns {number|Promise<number>}
   */
  xDeviceCharacteristics(pFile) {
    var _a;
    (_a = this["log"]) == null ? void 0 : _a.call(this, "jDeviceCharacteristics", pFile);
    return this.jDeviceCharacteristics(pFile);
  }
};
_FacadeVFS_instances = new WeakSet();
/**
 * Wrapped DataView for pointer arguments.
 * Pointers to a single value are passed using DataView. A Proxy
 * wrapper prevents use of incorrect type or endianness.
 * @param {'Int32'|'BigInt64'} type
 * @param {number} byteOffset
 * @returns {DataView}
 */
makeTypedDataView_fn = function(type, byteOffset) {
  const byteLength = type === "Int32" ? 4 : 8;
  const getter = `get${type}`;
  const setter = `set${type}`;
  const makeDataView = () => new DataView(this._module.HEAPU8.buffer, this._module.HEAPU8.byteOffset + byteOffset, byteLength);
  let dataView = makeDataView();
  return new Proxy(dataView, {
    get(_, prop) {
      if (dataView.buffer.byteLength === 0) {
        dataView = makeDataView();
      }
      if (prop === getter) {
        return function(byteOffset2, littleEndian) {
          if (!littleEndian)
            throw new Error("must be little endian");
          return dataView[prop](byteOffset2, littleEndian);
        };
      }
      if (prop === setter) {
        return function(byteOffset2, value, littleEndian) {
          if (!littleEndian)
            throw new Error("must be little endian");
          return dataView[prop](byteOffset2, value, littleEndian);
        };
      }
      if (typeof prop === "string" && /^(get)|(set)/.test(prop)) {
        throw new Error("invalid type");
      }
      const result = dataView[prop];
      return typeof result === "function" ? result.bind(dataView) : result;
    }
  });
};
/**
 * @param {number} byteOffset
 * @param {number} byteLength
 */
makeDataArray_fn = function(byteOffset, byteLength) {
  let target = this._module.HEAPU8.subarray(byteOffset, byteOffset + byteLength);
  return new Proxy(target, {
    get: (_, prop, receiver) => {
      if (target.buffer.byteLength === 0) {
        target = this._module.HEAPU8.subarray(byteOffset, byteOffset + byteLength);
      }
      const result = target[prop];
      return typeof result === "function" ? result.bind(target) : result;
    }
  });
};
decodeFilename_fn = function(zName, flags) {
  if (flags & VFS.SQLITE_OPEN_URI) {
    let pName = zName;
    let state = 1;
    const charCodes = [];
    while (state) {
      const charCode = this._module.HEAPU8[pName++];
      if (charCode) {
        charCodes.push(charCode);
      } else {
        if (!this._module.HEAPU8[pName])
          state = null;
        switch (state) {
          case 1: {
            charCodes.push("?".charCodeAt(0));
            state = 2;
            break;
          }
          case 2: {
            charCodes.push("=".charCodeAt(0));
            state = 3;
            break;
          }
          case 3: {
            charCodes.push("&".charCodeAt(0));
            state = 2;
            break;
          }
        }
      }
    }
    return new TextDecoder().decode(new Uint8Array(charCodes));
  }
  return zName ? this._module.UTF8ToString(zName) : null;
};
function delegalize(lo32, hi32) {
  return hi32 * 4294967296 + lo32 + (lo32 < 0 ? 2 ** 32 : 0);
}

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/browser/opfs/AccessHandlePoolVFS.js
var SECTOR_SIZE = 4096;
var HEADER_MAX_PATH_SIZE = 512;
var HEADER_FLAGS_SIZE = 4;
var HEADER_DIGEST_SIZE = 8;
var HEADER_CORPUS_SIZE = HEADER_MAX_PATH_SIZE + HEADER_FLAGS_SIZE;
var HEADER_OFFSET_FLAGS = HEADER_MAX_PATH_SIZE;
var HEADER_OFFSET_DIGEST = HEADER_CORPUS_SIZE;
var HEADER_OFFSET_DATA = SECTOR_SIZE;
var PERSISTENT_FILE_TYPES = VFS2.SQLITE_OPEN_MAIN_DB | VFS2.SQLITE_OPEN_MAIN_JOURNAL | VFS2.SQLITE_OPEN_SUPER_JOURNAL | VFS2.SQLITE_OPEN_WAL;
var DEFAULT_CAPACITY = 6;
var _directoryPath, _directoryHandle, _mapAccessHandleToName, _mapPathToAccessHandle, _availableAccessHandles, _mapIdToFile, _AccessHandlePoolVFS_instances, acquireAccessHandles_fn, releaseAccessHandles_fn, getAssociatedPath_fn, setAssociatedPath_fn, computeDigest_fn, getPath_fn, deletePath_fn;
var _AccessHandlePoolVFS = class _AccessHandlePoolVFS extends FacadeVFS {
  constructor(name, directoryPath, module) {
    super(name, module);
    __privateAdd(this, _AccessHandlePoolVFS_instances);
    __publicField(this, "log", null);
    //function(...args) { console.log(`[${contextName}]`, ...args) };
    // All the OPFS files the VFS uses are contained in one flat directory
    // specified in the constructor. No other files should be written here.
    __privateAdd(this, _directoryPath);
    __privateAdd(this, _directoryHandle);
    // The OPFS files all have randomly-generated names that do not match
    // the SQLite files whose data they contain. This map links those names
    // with their respective OPFS access handles.
    __privateAdd(this, _mapAccessHandleToName, /* @__PURE__ */ new Map());
    // When a SQLite file is associated with an OPFS file, that association
    // is kept in #mapPathToAccessHandle. Each access handle is in exactly
    // one of #mapPathToAccessHandle or #availableAccessHandles.
    __privateAdd(this, _mapPathToAccessHandle, /* @__PURE__ */ new Map());
    __privateAdd(this, _availableAccessHandles, /* @__PURE__ */ new Set());
    __privateAdd(this, _mapIdToFile, /* @__PURE__ */ new Map());
    __privateSet(this, _directoryPath, directoryPath);
  }
  static async create(name, directoryPath, module) {
    const vfs = new _AccessHandlePoolVFS(name, directoryPath, module);
    await vfs.isReady();
    return vfs;
  }
  getOpfsFileName(zName) {
    const path = __privateMethod(this, _AccessHandlePoolVFS_instances, getPath_fn).call(this, zName);
    const accessHandle = __privateGet(this, _mapPathToAccessHandle).get(path);
    return __privateGet(this, _mapAccessHandleToName).get(accessHandle);
  }
  resetAccessHandle(zName) {
    const path = __privateMethod(this, _AccessHandlePoolVFS_instances, getPath_fn).call(this, zName);
    const accessHandle = __privateGet(this, _mapPathToAccessHandle).get(path);
    accessHandle.truncate(HEADER_OFFSET_DATA);
  }
  jOpen(zName, fileId, flags, pOutFlags) {
    try {
      const path = zName ? __privateMethod(this, _AccessHandlePoolVFS_instances, getPath_fn).call(this, zName) : Math.random().toString(36);
      let accessHandle = __privateGet(this, _mapPathToAccessHandle).get(path);
      if (!accessHandle && flags & VFS2.SQLITE_OPEN_CREATE) {
        if (this.getSize() < this.getCapacity()) {
          ;
          [accessHandle] = __privateGet(this, _availableAccessHandles).keys();
          __privateMethod(this, _AccessHandlePoolVFS_instances, setAssociatedPath_fn).call(this, accessHandle, path, flags);
        } else {
          throw new Error("cannot create file");
        }
      }
      if (!accessHandle) {
        throw new Error("file not found");
      }
      const file = { path, flags, accessHandle };
      __privateGet(this, _mapIdToFile).set(fileId, file);
      pOutFlags.setInt32(0, flags, true);
      return VFS2.SQLITE_OK;
    } catch (e) {
      console.error(e.message);
      return VFS2.SQLITE_CANTOPEN;
    }
  }
  jClose(fileId) {
    const file = __privateGet(this, _mapIdToFile).get(fileId);
    if (file) {
      file.accessHandle.flush();
      __privateGet(this, _mapIdToFile).delete(fileId);
      if (file.flags & VFS2.SQLITE_OPEN_DELETEONCLOSE) {
        __privateMethod(this, _AccessHandlePoolVFS_instances, deletePath_fn).call(this, file.path);
      }
    }
    return VFS2.SQLITE_OK;
  }
  jRead(fileId, pData, iOffset) {
    const file = __privateGet(this, _mapIdToFile).get(fileId);
    const nBytes = file.accessHandle.read(pData.subarray(), { at: HEADER_OFFSET_DATA + iOffset });
    if (nBytes < pData.byteLength) {
      pData.fill(0, nBytes, pData.byteLength);
      return VFS2.SQLITE_IOERR_SHORT_READ;
    }
    return VFS2.SQLITE_OK;
  }
  jWrite(fileId, pData, iOffset) {
    const file = __privateGet(this, _mapIdToFile).get(fileId);
    const nBytes = file.accessHandle.write(pData.subarray(), { at: HEADER_OFFSET_DATA + iOffset });
    return nBytes === pData.byteLength ? VFS2.SQLITE_OK : VFS2.SQLITE_IOERR;
  }
  jTruncate(fileId, iSize) {
    const file = __privateGet(this, _mapIdToFile).get(fileId);
    file.accessHandle.truncate(HEADER_OFFSET_DATA + iSize);
    return VFS2.SQLITE_OK;
  }
  jSync(fileId, _flags) {
    const file = __privateGet(this, _mapIdToFile).get(fileId);
    file.accessHandle.flush();
    return VFS2.SQLITE_OK;
  }
  jFileSize(fileId, pSize64) {
    const file = __privateGet(this, _mapIdToFile).get(fileId);
    const size = file.accessHandle.getSize() - HEADER_OFFSET_DATA;
    pSize64.setBigInt64(0, BigInt(size), true);
    return VFS2.SQLITE_OK;
  }
  jSectorSize(_fileId) {
    return SECTOR_SIZE;
  }
  jDeviceCharacteristics(_fileId) {
    return VFS2.SQLITE_IOCAP_UNDELETABLE_WHEN_OPEN;
  }
  jAccess(zName, flags, pResOut) {
    const path = __privateMethod(this, _AccessHandlePoolVFS_instances, getPath_fn).call(this, zName);
    pResOut.setInt32(0, __privateGet(this, _mapPathToAccessHandle).has(path) ? 1 : 0, true);
    return VFS2.SQLITE_OK;
  }
  jDelete(zName, _syncDir) {
    const path = __privateMethod(this, _AccessHandlePoolVFS_instances, getPath_fn).call(this, zName);
    __privateMethod(this, _AccessHandlePoolVFS_instances, deletePath_fn).call(this, path);
    return VFS2.SQLITE_OK;
  }
  async close() {
    __privateMethod(this, _AccessHandlePoolVFS_instances, releaseAccessHandles_fn).call(this);
  }
  async isReady() {
    if (!__privateGet(this, _directoryHandle)) {
      let handle = await navigator.storage.getDirectory();
      for (const d of __privateGet(this, _directoryPath).split("/")) {
        if (d) {
          handle = await handle.getDirectoryHandle(d, { create: true });
        }
      }
      __privateSet(this, _directoryHandle, handle);
      await __privateMethod(this, _AccessHandlePoolVFS_instances, acquireAccessHandles_fn).call(this);
      if (this.getCapacity() === 0) {
        await this.addCapacity(DEFAULT_CAPACITY);
      }
    }
    return true;
  }
  /**
   * Returns the number of SQLite files in the file system.
   */
  getSize() {
    return __privateGet(this, _mapPathToAccessHandle).size;
  }
  /**
   * Returns the maximum number of SQLite files the file system can hold.
   */
  getCapacity() {
    return __privateGet(this, _mapAccessHandleToName).size;
  }
  /**
   * Increase the capacity of the file system by n.
   */
  async addCapacity(n) {
    for (let i = 0; i < n; ++i) {
      const name = Math.random().toString(36).replace("0.", "");
      const handle = await __privateGet(this, _directoryHandle).getFileHandle(name, { create: true });
      const accessHandle = await handle.createSyncAccessHandle();
      __privateGet(this, _mapAccessHandleToName).set(accessHandle, name);
      __privateMethod(this, _AccessHandlePoolVFS_instances, setAssociatedPath_fn).call(this, accessHandle, "", 0);
    }
    return n;
  }
  /**
   * Decrease the capacity of the file system by n. The capacity cannot be
   * decreased to fewer than the current number of SQLite files in the
   * file system.
   */
  async removeCapacity(n) {
    let nRemoved = 0;
    for (const accessHandle of Array.from(__privateGet(this, _availableAccessHandles))) {
      if (nRemoved == n || this.getSize() === this.getCapacity())
        return nRemoved;
      const name = __privateGet(this, _mapAccessHandleToName).get(accessHandle);
      accessHandle.close();
      await __privateGet(this, _directoryHandle).removeEntry(name);
      __privateGet(this, _mapAccessHandleToName).delete(accessHandle);
      __privateGet(this, _availableAccessHandles).delete(accessHandle);
      ++nRemoved;
    }
    return nRemoved;
  }
};
_directoryPath = new WeakMap();
_directoryHandle = new WeakMap();
_mapAccessHandleToName = new WeakMap();
_mapPathToAccessHandle = new WeakMap();
_availableAccessHandles = new WeakMap();
_mapIdToFile = new WeakMap();
_AccessHandlePoolVFS_instances = new WeakSet();
acquireAccessHandles_fn = async function() {
  const files = [];
  for await (const [name, handle] of __privateGet(this, _directoryHandle)) {
    if (handle.kind === "file") {
      files.push([name, handle]);
    }
  }
  await Promise.all(files.map(async ([name, handle]) => {
    const accessHandle = await handle.createSyncAccessHandle();
    __privateGet(this, _mapAccessHandleToName).set(accessHandle, name);
    const path = __privateMethod(this, _AccessHandlePoolVFS_instances, getAssociatedPath_fn).call(this, accessHandle);
    if (path) {
      __privateGet(this, _mapPathToAccessHandle).set(path, accessHandle);
    } else {
      __privateGet(this, _availableAccessHandles).add(accessHandle);
    }
  }));
};
releaseAccessHandles_fn = function() {
  for (const accessHandle of __privateGet(this, _mapAccessHandleToName).keys()) {
    accessHandle.close();
  }
  __privateGet(this, _mapAccessHandleToName).clear();
  __privateGet(this, _mapPathToAccessHandle).clear();
  __privateGet(this, _availableAccessHandles).clear();
};
/**
 * Read and return the associated path from an OPFS file header.
 * Empty string is returned for an unassociated OPFS file.
 * @returns {string} path or empty string
 */
getAssociatedPath_fn = function(accessHandle) {
  const corpus = new Uint8Array(HEADER_CORPUS_SIZE);
  accessHandle.read(corpus, { at: 0 });
  const dataView = new DataView(corpus.buffer, corpus.byteOffset);
  const flags = dataView.getUint32(HEADER_OFFSET_FLAGS);
  if (corpus[0] && (flags & VFS2.SQLITE_OPEN_DELETEONCLOSE || (flags & PERSISTENT_FILE_TYPES) === 0)) {
    console.warn(`Remove file with unexpected flags ${flags.toString(16)}`);
    __privateMethod(this, _AccessHandlePoolVFS_instances, setAssociatedPath_fn).call(this, accessHandle, "", 0);
    return "";
  }
  const fileDigest = new Uint32Array(HEADER_DIGEST_SIZE / 4);
  accessHandle.read(fileDigest, { at: HEADER_OFFSET_DIGEST });
  const computedDigest = __privateMethod(this, _AccessHandlePoolVFS_instances, computeDigest_fn).call(this, corpus);
  if (fileDigest.every((value, i) => value === computedDigest[i])) {
    const pathBytes = corpus.indexOf(0);
    if (pathBytes === 0) {
      accessHandle.truncate(HEADER_OFFSET_DATA);
    }
    return new TextDecoder().decode(corpus.subarray(0, pathBytes));
  } else {
    console.warn("Disassociating file with bad digest.");
    __privateMethod(this, _AccessHandlePoolVFS_instances, setAssociatedPath_fn).call(this, accessHandle, "", 0);
    return "";
  }
};
/**
 * Set the path on an OPFS file header.
 */
setAssociatedPath_fn = function(accessHandle, path, flags) {
  const corpus = new Uint8Array(HEADER_CORPUS_SIZE);
  const encodedResult = new TextEncoder().encodeInto(path, corpus);
  if (encodedResult.written >= HEADER_MAX_PATH_SIZE) {
    throw new Error("path too long");
  }
  const dataView = new DataView(corpus.buffer, corpus.byteOffset);
  dataView.setUint32(HEADER_OFFSET_FLAGS, flags);
  const digest = __privateMethod(this, _AccessHandlePoolVFS_instances, computeDigest_fn).call(this, corpus);
  accessHandle.write(corpus, { at: 0 });
  accessHandle.write(digest, { at: HEADER_OFFSET_DIGEST });
  accessHandle.flush();
  if (path) {
    __privateGet(this, _mapPathToAccessHandle).set(path, accessHandle);
    __privateGet(this, _availableAccessHandles).delete(accessHandle);
  } else {
    accessHandle.truncate(HEADER_OFFSET_DATA);
    __privateGet(this, _availableAccessHandles).add(accessHandle);
  }
};
/**
 * We need a synchronous digest function so can't use WebCrypto.
 * Adapted from https://github.com/bryc/code/blob/master/jshash/experimental/cyrb53.js
 * @returns {ArrayBuffer} 64-bit digest
 */
computeDigest_fn = function(corpus) {
  if (!corpus[0]) {
    return new Uint32Array([4274806656, 2899230775]);
  }
  let h1 = 3735928559;
  let h2 = 1103547991;
  for (const value of corpus) {
    h1 = Math.imul(h1 ^ value, 2654435761);
    h2 = Math.imul(h2 ^ value, 1597334677);
  }
  h1 = Math.imul(h1 ^ h1 >>> 16, 2246822507) ^ Math.imul(h2 ^ h2 >>> 13, 3266489909);
  h2 = Math.imul(h2 ^ h2 >>> 16, 2246822507) ^ Math.imul(h1 ^ h1 >>> 13, 3266489909);
  return new Uint32Array([h1 >>> 0, h2 >>> 0]);
};
/**
 * Convert a bare filename, path, or URL to a UNIX-style path.
 */
getPath_fn = function(nameOrURL) {
  const url = typeof nameOrURL === "string" ? new URL(nameOrURL, "file://localhost/") : nameOrURL;
  return url.pathname;
};
/**
 * Remove the association between a path and an OPFS file.
 * @param {string} path
 */
deletePath_fn = function(path) {
  const accessHandle = __privateGet(this, _mapPathToAccessHandle).get(path);
  if (accessHandle) {
    __privateGet(this, _mapPathToAccessHandle).delete(path);
    __privateMethod(this, _AccessHandlePoolVFS_instances, setAssociatedPath_fn).call(this, accessHandle, "", 0);
  }
};
var AccessHandlePoolVFS = _AccessHandlePoolVFS;

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/browser/opfs/index.js
var semaphore = Effect_exports.makeSemaphore(1).pipe(Effect_exports.runSync);
var opfsVfsMap = /* @__PURE__ */ new Map();
var makeOpfsDb = ({ sqlite3, directory, fileName }) => Effect_exports.gen(function* () {
  const safePath = directory.replaceAll(/["*/:<>?\\|]/g, "_");
  const pathSegment = safePath.length === 0 ? "" : `-${safePath}`;
  const vfsName = `opfs${pathSegment}`;
  if (sqlite3.vfs_registered.has(vfsName) === false) {
    const vfs2 = yield* Effect_exports.promise(() => AccessHandlePoolVFS.create(vfsName, directory, sqlite3.module));
    sqlite3.vfs_register(vfs2, false);
    opfsVfsMap.set(vfsName, vfs2);
  }
  const dbPointer = sqlite3.open_v2Sync(fileName, void 0, vfsName);
  const vfs = opfsVfsMap.get(vfsName);
  return { dbPointer, vfs };
}).pipe(semaphore.withPermits(1));

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/browser/opfs/opfs-sah-pool.js
import * as VFS3 from "@livestore/wa-sqlite/src/VFS.js";
var SECTOR_SIZE2 = 4096;
var HEADER_MAX_PATH_SIZE2 = 512;
var HEADER_FLAGS_SIZE2 = 4;
var HEADER_DIGEST_SIZE2 = 8;
var HEADER_CORPUS_SIZE2 = HEADER_MAX_PATH_SIZE2 + HEADER_FLAGS_SIZE2;
var HEADER_OFFSET_FLAGS2 = HEADER_MAX_PATH_SIZE2;
var HEADER_OFFSET_DIGEST2 = HEADER_CORPUS_SIZE2;
var HEADER_OFFSET_DATA2 = SECTOR_SIZE2;
var PERSISTENT_FILE_TYPES2 = VFS3.SQLITE_OPEN_MAIN_DB | VFS3.SQLITE_OPEN_MAIN_JOURNAL | VFS3.SQLITE_OPEN_SUPER_JOURNAL | VFS3.SQLITE_OPEN_WAL;
var textDecoder = new TextDecoder();
var decodeSAHPoolFilename = async (file) => {
  const corpus = new Uint8Array(await file.slice(0, HEADER_CORPUS_SIZE2).arrayBuffer());
  const dataView = new DataView(corpus.buffer, corpus.byteOffset);
  const flags = dataView.getUint32(HEADER_OFFSET_FLAGS2);
  if (corpus[0] && (flags & VFS3.SQLITE_OPEN_DELETEONCLOSE || (flags & PERSISTENT_FILE_TYPES2) === 0)) {
    console.warn(`Remove file with unexpected flags ${flags.toString(16)}`);
    return "";
  }
  const fileDigest = new Uint32Array(await file.slice(HEADER_OFFSET_DIGEST2, HEADER_OFFSET_DIGEST2 + HEADER_DIGEST_SIZE2).arrayBuffer());
  const computedDigest = computeDigest(corpus);
  if (fileDigest.every((value, i) => value === computedDigest[i])) {
    const pathBytes = corpus.indexOf(0);
    if (pathBytes === 0) {
    }
    return textDecoder.decode(corpus.subarray(0, pathBytes));
  } else {
    console.warn("Disassociating file with bad digest.");
    return "";
  }
};
var computeDigest = (corpus) => {
  if (!corpus[0]) {
    return new Uint32Array([4274806656, 2899230775]);
  }
  let h1 = 3735928559;
  let h2 = 1103547991;
  for (const value of corpus) {
    h1 = Math.imul(h1 ^ value, 2654435761);
    h2 = Math.imul(h2 ^ value, 1597334677);
  }
  h1 = Math.imul(h1 ^ h1 >>> 16, 2246822507) ^ Math.imul(h2 ^ h2 >>> 13, 3266489909);
  h2 = Math.imul(h2 ^ h2 >>> 16, 2246822507) ^ Math.imul(h1 ^ h1 >>> 13, 3266489909);
  return new Uint32Array([h1 >>> 0, h2 >>> 0]);
};

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/browser/mod.js
var sqliteDbFactory = ({ sqlite3 }) => (input) => Effect_exports.gen(function* () {
  if (input._tag === "in-memory") {
    const { dbPointer: dbPointer2, vfs: vfs2 } = makeInMemoryDb(sqlite3);
    return makeSqliteDb({
      sqlite3,
      metadata: {
        _tag: "in-memory",
        vfs: vfs2,
        dbPointer: dbPointer2,
        deleteDb: () => {
        },
        configureDb: input.configureDb ?? (() => {
        }),
        persistenceInfo: {
          fileName: ":memory:"
        }
      }
    });
  }
  const MAX_DB_FILENAME_LENGTH = 60;
  let dbFilename = input.fileName;
  if (input.fileName.length > MAX_DB_FILENAME_LENGTH) {
    yield* Effect_exports.logWarning(`dbFilename too long: '${input.fileName}'. Max ${MAX_DB_FILENAME_LENGTH} chars, got ${input.fileName.length}. Hashing...`);
    dbFilename = `hash-${Hash_exports.string(input.fileName)}.db`;
  }
  const { dbPointer, vfs } = yield* makeOpfsDb({
    sqlite3,
    directory: input.opfsDirectory,
    fileName: dbFilename
  });
  return makeSqliteDb({
    sqlite3,
    metadata: {
      _tag: "opfs",
      vfs,
      dbPointer,
      deleteDb: () => vfs.resetAccessHandle(input.fileName),
      configureDb: input.configureDb ?? (() => {
      }),
      persistenceInfo: {
        fileName: dbFilename,
        opfsDirectory: input.opfsDirectory,
        opfsFileName: vfs.getOpfsFileName(dbFilename)
      }
    }
  });
});

// node_modules/.pnpm/@livestore+sqlite-wasm@0.3.1_a2602a20d83ff4e94e4a35c6aa299805/node_modules/@livestore/sqlite-wasm/dist/load-wasm/mod.browser.js
import * as WaSqlite from "@livestore/wa-sqlite";
import WaSqliteFactory from "@livestore/wa-sqlite/dist/wa-sqlite.mjs";
var loadSqlite3Wasm = async () => {
  const module = await WaSqliteFactory();
  const sqlite3 = WaSqlite.Factory(module);
  sqlite3.module = module;
  return sqlite3;
};

// node_modules/.pnpm/@livestore+adapter-web@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/adapter-web/dist/opfs-utils.js
var opfs_utils_exports = {};
__export(opfs_utils_exports, {
  deleteAll: () => deleteAll,
  getDirHandle: () => getDirHandle,
  printTree: () => printTree,
  rootHandlePromise: () => rootHandlePromise
});
var rootHandlePromise = typeof navigator === "undefined" || navigator.storage === void 0 ? (
  // We're using a proxy here to make the promise reject lazy
  new Proxy({}, {
    get: () => Promise.reject(new Error(`Can't get OPFS root handle in this environment as navigator.storage is undefined`))
  })
) : navigator.storage.getDirectory();
var getDirHandle = async (absDirPath) => {
  const rootHandle = await rootHandlePromise;
  if (absDirPath === void 0)
    return rootHandle;
  let dirHandle = rootHandle;
  const directoryStack = absDirPath == null ? void 0 : absDirPath.split("/").filter(Boolean);
  while (directoryStack.length > 0) {
    dirHandle = await dirHandle.getDirectoryHandle(directoryStack.shift());
  }
  return dirHandle;
};
var printTree = async (directoryHandle_ = rootHandlePromise, depth = Number.POSITIVE_INFINITY, prefix = "") => {
  if (depth < 0)
    return;
  const directoryHandle = await directoryHandle_;
  const entries = directoryHandle.values();
  for await (const entry of entries) {
    const isDirectory = entry.kind === "directory";
    const size = entry.kind === "file" ? await entry.getFile().then((file) => prettyBytes(file.size)) : void 0;
    console.log(`${prefix}${isDirectory ? "📁" : "📄"} ${entry.name} ${size ? `(${size})` : ""}`);
    if (isDirectory) {
      const nestedDirectoryHandle = await directoryHandle.getDirectoryHandle(entry.name);
      await printTree(nestedDirectoryHandle, depth - 1, `${prefix}  `);
    }
  }
};
var deleteAll = async (directoryHandle) => {
  if (directoryHandle.kind !== "directory")
    return;
  for await (const entryName of directoryHandle.keys()) {
    await directoryHandle.removeEntry(entryName, { recursive: true });
  }
};

// node_modules/.pnpm/@livestore+adapter-web@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/adapter-web/dist/web-worker/common/persisted-sqlite.js
var PersistedSqliteError = class extends Schema_exports.TaggedError()("PersistedSqliteError", {
  cause: Schema_exports.Defect
}) {
};
var readPersistedAppDbFromClientSession = ({ storageOptions, storeId, schema }) => Effect_exports.promise(async () => {
  const directory = sanitizeOpfsDir(storageOptions.directory, storeId);
  const sahPoolOpaqueDir = await getDirHandle(directory).catch(() => void 0);
  if (sahPoolOpaqueDir === void 0) {
    return void 0;
  }
  const tryGetDbFile = async (fileHandle) => {
    const file = await fileHandle.getFile();
    const fileName = await decodeSAHPoolFilename(file);
    return fileName ? { fileName, file } : void 0;
  };
  const getAllFiles = async (asyncIterator) => {
    const results = [];
    for await (const value of asyncIterator) {
      if (value.kind === "file") {
        results.push(value);
      }
    }
    return results;
  };
  const files = await getAllFiles(sahPoolOpaqueDir.values());
  const fileResults = await Promise.all(files.map(tryGetDbFile));
  const appDbFileName = "/" + getStateDbFileName(schema);
  const dbFileRes = fileResults.find((_) => (_ == null ? void 0 : _.fileName) === appDbFileName);
  if (dbFileRes !== void 0) {
    const data = await dbFileRes.file.slice(HEADER_OFFSET_DATA2).arrayBuffer();
    if (data.byteLength === 0) {
      return void 0;
    }
    return new Uint8Array(data);
  }
  return void 0;
}).pipe(Effect_exports.logWarnIfTakesLongerThan({
  duration: 1e3,
  label: "@livestore/adapter-web:readPersistedAppDbFromClientSession"
}), Effect_exports.withPerformanceMeasure("@livestore/adapter-web:readPersistedAppDbFromClientSession"), Effect_exports.withSpan("@livestore/adapter-web:readPersistedAppDbFromClientSession"));
var resetPersistedDataFromClientSession = ({ storageOptions, storeId }) => Effect_exports.gen(function* () {
  const directory = sanitizeOpfsDir(storageOptions.directory, storeId);
  yield* opfsDeleteAbs(directory);
}).pipe(Effect_exports.retry({
  schedule: Schedule_exports.exponentialBackoff10Sec
}), Effect_exports.withSpan("@livestore/adapter-web:resetPersistedDataFromClientSession"));
var opfsDeleteAbs = (absPath) => Effect_exports.promise(async () => {
  const root = await rootHandlePromise;
  const pathParts = absPath.split("/").filter((part) => part.length);
  try {
    let currentDir = root;
    for (let i = 0; i < pathParts.length - 1; i++) {
      currentDir = await currentDir.getDirectoryHandle(pathParts[i]);
    }
    await currentDir.removeEntry(pathParts.at(-1), { recursive: true });
  } catch (error) {
    if (error instanceof DOMException && error.name === "NotFoundError") {
      return;
    } else {
      throw error;
    }
  }
}).pipe(UnexpectedError.mapToUnexpectedError, Effect_exports.withSpan("@livestore/adapter-web:worker:opfsDeleteFile", { attributes: { absFilePath: absPath } }));
var sanitizeOpfsDir = (directory, storeId) => {
  if (directory === void 0 || directory === "" || directory === "/")
    return `livestore-${storeId}@${liveStoreStorageFormatVersion}`;
  if (directory.includes("/")) {
    throw new Error(`@livestore/adapter-web:worker:sanitizeOpfsDir: Nested directories are not yet supported ('${directory}')`);
  }
  return `${directory}@${liveStoreStorageFormatVersion}`;
};
var getStateDbFileName = (schema) => {
  const schemaHashSuffix = schema.state.sqlite.migrations.strategy === "manual" ? "fixed" : schema.state.sqlite.hash.toString();
  return `state${schemaHashSuffix}.db`;
};

// node_modules/.pnpm/@livestore+adapter-web@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/adapter-web/dist/web-worker/common/shutdown-channel.js
var makeShutdownChannel = (storeId) => mod_exports.broadcastChannel({
  channelName: `livestore.shutdown.${storeId}`,
  schema: shutdown_channel_exports.All
});

// node_modules/.pnpm/@livestore+adapter-web@0.3.1_505c75a1204e4cf21acbbd6b3e225a6b/node_modules/@livestore/adapter-web/dist/web-worker/common/worker-schema.js
var worker_schema_exports = {};
__export(worker_schema_exports, {
  LeaderWorkerInner: () => LeaderWorkerInner,
  LeaderWorkerOuter: () => LeaderWorkerOuter,
  SharedWorker: () => SharedWorker,
  StorageType: () => StorageType,
  StorageTypeOpfs: () => StorageTypeOpfs,
  SyncBackendOptions: () => SyncBackendOptions
});
var StorageTypeOpfs = Schema_exports.Struct({
  type: Schema_exports.Literal("opfs"),
  /**
   * Default is `livestore-${storeId}`
   *
   * When providing this option, make sure to include the `storeId` in the path to avoid
   * conflicts with other LiveStore apps.
   */
  directory: Schema_exports.optional(Schema_exports.String)
});
var StorageType = Schema_exports.Union(StorageTypeOpfs);
var SyncBackendOptions = Schema_exports.Record({ key: Schema_exports.String, value: Schema_exports.JsonValue });
var LeaderWorkerOuter;
(function(LeaderWorkerOuter2) {
  class InitialMessage extends Schema_exports.TaggedRequest()("InitialMessage", {
    payload: { port: Transferable_exports.MessagePort, storeId: Schema_exports.String, clientId: Schema_exports.String },
    success: Schema_exports.Void,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerOuter2.InitialMessage = InitialMessage;
  class Request2 extends Schema_exports.Union(InitialMessage) {
  }
  LeaderWorkerOuter2.Request = Request2;
})(LeaderWorkerOuter || (LeaderWorkerOuter = {}));
var LeaderWorkerInner;
(function(LeaderWorkerInner2) {
  class InitialMessage extends Schema_exports.TaggedRequest()("InitialMessage", {
    payload: {
      storageOptions: StorageType,
      devtoolsEnabled: Schema_exports.Boolean,
      storeId: Schema_exports.String,
      clientId: Schema_exports.String,
      debugInstanceId: Schema_exports.String,
      syncPayload: Schema_exports.UndefinedOr(Schema_exports.JsonValue)
    },
    success: Schema_exports.Void,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.InitialMessage = InitialMessage;
  class BootStatusStream extends Schema_exports.TaggedRequest()("BootStatusStream", {
    payload: {},
    success: BootStatus,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.BootStatusStream = BootStatusStream;
  class PushToLeader extends Schema_exports.TaggedRequest()("PushToLeader", {
    payload: {
      batch: Schema_exports.Array(LiveStoreEvent_exports.AnyEncoded)
    },
    success: Schema_exports.Void,
    failure: Schema_exports.Union(UnexpectedError, LeaderAheadError)
  }) {
  }
  LeaderWorkerInner2.PushToLeader = PushToLeader;
  class PullStream extends Schema_exports.TaggedRequest()("PullStream", {
    payload: {
      cursor: LeaderPullCursor
    },
    success: Schema_exports.Struct({
      payload: syncstate_exports.PayloadUpstream,
      mergeCounter: Schema_exports.Number
    }),
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.PullStream = PullStream;
  class Export extends Schema_exports.TaggedRequest()("Export", {
    payload: {},
    success: Transferable_exports.Uint8Array,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.Export = Export;
  class ExportEventlog extends Schema_exports.TaggedRequest()("ExportEventlog", {
    payload: {},
    success: Transferable_exports.Uint8Array,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.ExportEventlog = ExportEventlog;
  class GetRecreateSnapshot extends Schema_exports.TaggedRequest()("GetRecreateSnapshot", {
    payload: {},
    success: Schema_exports.Struct({
      snapshot: Transferable_exports.Uint8Array,
      migrationsReport: MigrationsReport
    }),
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.GetRecreateSnapshot = GetRecreateSnapshot;
  class GetLeaderHead extends Schema_exports.TaggedRequest()("GetLeaderHead", {
    payload: {},
    success: EventSequenceNumber_exports.EventSequenceNumber,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.GetLeaderHead = GetLeaderHead;
  class GetLeaderSyncState extends Schema_exports.TaggedRequest()("GetLeaderSyncState", {
    payload: {},
    success: syncstate_exports.SyncState,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.GetLeaderSyncState = GetLeaderSyncState;
  class Shutdown extends Schema_exports.TaggedRequest()("Shutdown", {
    payload: {},
    success: Schema_exports.Void,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.Shutdown = Shutdown;
  class ExtraDevtoolsMessage extends Schema_exports.TaggedRequest()("ExtraDevtoolsMessage", {
    payload: {
      message: mod_exports2.Leader.MessageToApp
    },
    success: Schema_exports.Void,
    failure: UnexpectedError
  }) {
  }
  LeaderWorkerInner2.ExtraDevtoolsMessage = ExtraDevtoolsMessage;
  LeaderWorkerInner2.Request = Schema_exports.Union(InitialMessage, BootStatusStream, PushToLeader, PullStream, Export, ExportEventlog, GetRecreateSnapshot, GetLeaderHead, GetLeaderSyncState, Shutdown, ExtraDevtoolsMessage, schema_exports.CreateConnection);
})(LeaderWorkerInner || (LeaderWorkerInner = {}));
var SharedWorker;
(function(SharedWorker2) {
  class InitialMessagePayloadFromClientSession extends Schema_exports.TaggedStruct("FromClientSession", {
    initialMessage: LeaderWorkerInner.InitialMessage
  }) {
  }
  SharedWorker2.InitialMessagePayloadFromClientSession = InitialMessagePayloadFromClientSession;
  class InitialMessage extends Schema_exports.TaggedRequest()("InitialMessage", {
    payload: {
      payload: Schema_exports.Union(InitialMessagePayloadFromClientSession, Schema_exports.TaggedStruct("FromWebBridge", {})),
      // To guard against scenarios where a client session is already running a newer version of LiveStore
      // We should probably find a better way to handle those cases once they become more common.
      liveStoreVersion: Schema_exports.Literal(liveStoreVersion)
    },
    success: Schema_exports.Void,
    failure: UnexpectedError
  }) {
  }
  SharedWorker2.InitialMessage = InitialMessage;
  class UpdateMessagePort extends Schema_exports.TaggedRequest()("UpdateMessagePort", {
    payload: {
      port: Transferable_exports.MessagePort
    },
    success: Schema_exports.Void,
    failure: UnexpectedError
  }) {
  }
  SharedWorker2.UpdateMessagePort = UpdateMessagePort;
  class Request2 extends Schema_exports.Union(
    InitialMessage,
    UpdateMessagePort,
    // Proxied requests
    LeaderWorkerInner.BootStatusStream,
    LeaderWorkerInner.PushToLeader,
    LeaderWorkerInner.PullStream,
    LeaderWorkerInner.Export,
    LeaderWorkerInner.GetRecreateSnapshot,
    LeaderWorkerInner.ExportEventlog,
    LeaderWorkerInner.GetLeaderHead,
    LeaderWorkerInner.GetLeaderSyncState,
    LeaderWorkerInner.Shutdown,
    LeaderWorkerInner.ExtraDevtoolsMessage,
    schema_exports.CreateConnection
  ) {
  }
  SharedWorker2.Request = Request2;
})(SharedWorker || (SharedWorker = {}));

export {
  configureConnection,
  LeaderThreadCtx,
  eventlog_exports,
  makeLeaderThreadLayer,
  CreateConnection,
  schema_exports,
  CacheService,
  CreateConnection2,
  sqliteDbFactory,
  loadSqlite3Wasm,
  opfs_utils_exports,
  readPersistedAppDbFromClientSession,
  resetPersistedDataFromClientSession,
  sanitizeOpfsDir,
  getStateDbFileName,
  makeShutdownChannel,
  StorageType,
  LeaderWorkerOuter,
  LeaderWorkerInner,
  SharedWorker,
  worker_schema_exports
};
//# sourceMappingURL=chunk-OFAEZEPS.js.map
